% =================================================================
% CHAPTER 4: EXPERIMENTS
% =================================================================
\chapter{Experiments}
\label{ch:experiments}

% -----------------------------------------------------------------
\section{Implementation Details}
\label{sec:implementation}

\subsection{Hardware and Software}
\label{subsec:hardware}

All experiments were conducted on a compute cluster managed by SLURM, using NVIDIA GPUs with up to 48~GB VRAM (A6000 / A5000 class). The software stack includes PyTorch~2.x with CUDA~12.1, the HuggingFace Diffusers library~\cite{von-platen-etal-2022-diffusers} for diffusion model components, and Weights~\&~Biases for experiment tracking and visualization. Training was performed in mixed precision (FP16) with gradient checkpointing enabled to reduce GPU memory consumption.

\subsection{Semantic VAE Training}
\label{subsec:vae_training}

Table~\ref{tab:vae_config} summarizes the Semantic VAE training configuration.

\begin{table}[t]
    \centering
    \caption{Semantic VAE training configuration}
    \label{tab:vae_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Pretrained VAE & \texttt{stabilityai/stable-video-diffusion-img2vid-xt} \\
        Input resolution & $192 \times 704$ \\
        Number of classes & 19 (Cityscapes label set) \\
        Clip size & 4 frames \\
        Batch size & 1 clip (4 frames) \\
        Training clips & 500 (from KITTI-360 training set) \\
        Validation clips & 200 \\
        Optimizer & AdamW~\cite{loshchilov2019decoupled} ($\beta_1\!=\!0.9$, $\beta_2\!=\!0.999$) \\
        Learning rate & $1 \times 10^{-3}$ \\
        Weight decay & $1 \times 10^{-2}$ \\
        LR scheduler & Cosine annealing (min LR $= 10^{-6}$) \\
        Warmup steps & 500 \\
        Max epochs & 50 \\
        Early stopping patience & 10 epochs (on val mIoU) \\
        Boundary emphasis $\alpha$ & 4.0 \\
        Dice weight $\lambda_{\text{Dice}}$ & 0.5 \\
        Stem hidden dim & 64 \\
        Head hidden dim & 64 \\
        Trainable parameters & $\sim$200K \\
        Frozen VAE parameters & $\sim$84M \\
        Training time & $\sim$4--6 hours \\
        \bottomrule
    \end{tabular}
\end{table}

The relatively high learning rate of $10^{-3}$ is justified by the small number of trainable parameters ($\sim$200K), which allows aggressive optimization without destabilizing the frozen VAE core. The cosine annealing schedule with warm restarts ensures smooth convergence, while early stopping on validation mIoU prevents overfitting to the limited training subset.

\subsection{Stage~1: Semantic Video Prediction}
\label{subsec:stage1_training}

Table~\ref{tab:stage1_config} summarizes the Stage~1 training configuration.

\begin{table}[t]
    \centering
    \caption{Stage~1 (Semantic Prediction) training configuration}
    \label{tab:stage1_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base model & SVD-XT (\texttt{stable-video-diffusion-img2vid-xt}) \\
        Input resolution & $192 \times 704$ \\
        Clip length & 25 frames \\
        Latent shape per clip & $25 \times 4 \times 24 \times 88$ \\
        Batch size & 1 \\
        Gradient accumulation & 6 steps (effective batch size 6) \\
        Learning rate & $5 \times 10^{-6}$ \\
        LR scheduler & Constant \\
        Optimizer & AdamW~\cite{loshchilov2019decoupled} \\
        Mixed precision & FP16 \\
        Guidance scale range (train) & $[3.0,\, 7.0]$ \\
        Noise augmentation strength & 0.01 \\
        Conditioning dropout prob. & 0.1 \\
        Number of conditioning frames & 1 \\
        Inference steps (validation) & 30 \\
        Epochs & 10 \\
        Checkpointing interval & Every 200 steps \\
        Validation interval & Every 500 steps \\
        Seed & 1234 \\
        Data workers & 8 \\
        Training time & $\sim$24--48 hours \\
        \bottomrule
    \end{tabular}
\end{table}

The semantic conditioning pathway loads semantic label maps encoded via the frozen Semantic VAE, replacing the bounding box overlays used in the original Ctrl-V framework.

\subsection{Stage~2: Semantic-to-Video Generation}
\label{subsec:stage2_training}

Table~\ref{tab:stage2_config} summarizes the Stage~2 training configuration. Notably, Stage~2 can be trained in parallel with Stage~1 because ground-truth semantic labels (not Stage~1 predictions) are used for conditioning during training.

\begin{table}[t]
    \centering
    \caption{Stage~2 (Semantic-to-Video) training configuration}
    \label{tab:stage2_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base model & SVD-XT (\texttt{stable-video-diffusion-img2vid-xt}) \\
        Conditioning architecture & ControlNet (copy of SVD UNet encoder) \\
        Input resolution & $192 \times 704$ \\
        Clip length & 25 frames \\
        Batch size & 1 \\
        Gradient accumulation & 4 steps (effective batch size 4) \\
        Learning rate & $1 \times 10^{-5}$ \\
        LR scheduler & Constant \\
        Optimizer & AdamW~\cite{loshchilov2019decoupled} \\
        Mixed precision & FP16 \\
        Guidance scale range (train) & $[1.0,\, 3.0]$ \\
        Noise augmentation strength & 0.01 \\
        Conditioning dropout prob. & 0.1 \\
        Inference steps (validation) & 30 \\
        Epochs & 10 \\
        Checkpointing interval & Every 100 steps \\
        Validation interval & Every 300 steps \\
        Trainable components & ControlNet weights only \\
        Frozen components & SVD UNet backbone \\
        Seed & 1234 \\
        Data workers & 8 \\
        Training time & $\sim$18--36 hours \\
        \bottomrule
    \end{tabular}
\end{table}

The lower guidance scale range ($[1.0, 3.0]$ vs. $[3.0, 7.0]$ in Stage~1) reflects the fact that Stage~2 receives strong spatial conditioning from the semantic maps via ControlNet, reducing the need for aggressive classifier-free guidance.

\subsection{UNet Architecture Details}
\label{subsec:unet_details}

Both stages use the SVD-XT UNet as the denoising backbone. Table~\ref{tab:unet_arch} summarizes the key architectural parameters.

\begin{table}[t]
    \centering
    \caption{SVD-XT UNet architecture summary}
    \label{tab:unet_arch}
    \begin{tabular}{ll}
        \toprule
        \textbf{Component} & \textbf{Details} \\
        \midrule
        Base channels & 320 \\
        Channel multipliers & [1, 2, 4, 4] \\
        Attention resolutions & 16, 8 (spatial) \\
        Spatial attention heads & 5, 10, 20, 20 \\
        Temporal attention & At each attention resolution \\
        Temporal convolutions & At each resolution level \\
        Down blocks & 4 (CrossAttn + Downsample) \\
        Mid block & CrossAttn \\
        Up blocks & 4 (CrossAttn + Upsample) \\
        Conditioning & CLIP image features (cross-attention) \\
        Latent channels & 4 (input), 8 (with concat conditioning) \\
        Total parameters (UNet) & $\sim$1.5B \\
        Total parameters (ControlNet) & $\sim$400M \\
        \bottomrule
    \end{tabular}
\end{table}

% -----------------------------------------------------------------
\section{Semantic VAE Results}
\label{sec:vae_results}

\subsection{Progressive Architecture Development}
\label{subsec:vae_phases}

The Semantic VAE was developed through iterative refinement across three phases, summarized in Table~\ref{tab:vae_phases}.

\begin{table}[t]
    \centering
    \caption{Semantic VAE development phases and results (100 validation samples)}
    \label{tab:vae_phases}
    \begin{tabular}{llccr}
        \toprule
        \textbf{Phase} & \textbf{Approach} & \textbf{mIoU} & \textbf{Pixel Acc.} & \textbf{Trainable Params} \\
        \midrule
        1 & RGB VAE baseline (no training) & 54.3\% & 97.7\% & 0 \\
        2a & Adapter (no boundary weight) & 64.8\% & 93.8\% & 14K \\
        2a & Adapter ($\alpha=8.0$) & 79.5\% & 99.3\% & 14K \\
        2b & Native (CE only, $\alpha=4.0$) & 88.0\% & 99.1\% & 200K \\
        \textbf{2b} & \textbf{Native ($\alpha=4.0$, $\lambda=0.5$)} & \textbf{89.7\%} & \textbf{99.0\%} & \textbf{200K} \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Phase~1: RGB VAE Baseline.}
Semantic IDs were converted to an RGB color palette, passed through the frozen SVD VAE, and decoded back via nearest-neighbor color matching. The 97.7\% pixel accuracy but only 54.3\% mIoU reveals a critical insight: the pretrained VAE's smooth latent space destroys semantic boundaries, which account for less than 10\% of all pixels but contribute to over 40\% of errors. Large homogeneous regions (road, building, vegetation) are reconstructed accurately, while thin structures and class transitions are severely degraded.

\paragraph{Phase~2a: Adapter Training.}
Lightweight adapter modules (a $1 \times 1$ convolution from 19 to 3 channels at the input and 3 to 19 channels at the output, totaling 14K parameters) were trained around the frozen VAE. Without boundary weighting, the model achieved 64.8\% mIoU---actually \emph{lower} than the baseline in pixel accuracy due to training instabilities. Adding boundary-weighted cross-entropy ($\alpha=8.0$) dramatically improved results to 79.5\% mIoU, a 22.7\% absolute improvement. This demonstrated that boundary emphasis is the single most impactful loss design choice.

\paragraph{Phase~2b: Semantic-Native Architecture.}
Removing the 3-channel RGB bottleneck by directly feeding 128-channel semantic features into the VAE core further improved performance to 88.0\% mIoU (with CE loss only). Adding Dice loss ($\lambda_{\text{Dice}}=0.5$) yielded the final best result of \textbf{89.7\% mIoU}. The 35.4\% absolute improvement from baseline to final model validates both architectural and loss function innovations.

\subsection{Per-Class IoU Analysis}
\label{subsec:per_class_iou}

Table~\ref{tab:per_class_iou} presents the per-class IoU for selected configurations.

\begin{table}[t]
    \centering
    \caption{Per-class IoU comparison across Semantic VAE configurations}
    \label{tab:per_class_iou}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Class} & \textbf{Phase~1} & \textbf{Phase~2a} & \textbf{Phase~2b} \\
        & (RGB Baseline) & (Adapter + Boundary) & (Native + Dice) \\
        \midrule
        Road       & 55\% & 99.6\% & 99.6\% \\
        Sidewalk   & 60\% & 97.0\% & 98.3\% \\
        Building   & 80\% & 99.1\% & 98.5\% \\
        Wall       & 75\% & 97.1\% & 95.1\% \\
        Fence      & 70\% & 96.8\% & 94.8\% \\
        Pole       & 12\% & 46.5\% & \textbf{84.0\%} \\
        Traffic Light & 10\% & 12.0\% & 78.5\% \\
        Traffic Sign & 8\% & 0.0\% & \textbf{82.2\%} \\
        Vegetation & 85\% & 99.3\% & 98.2\% \\
        Terrain    & 30\% & 98.0\% & 97.7\% \\
        Sky        & 60\% & 97.4\% & 97.8\% \\
        Person     & 15\% & 40.0\% & 75.0\% \\
        Rider      & 10\% & 5.0\% & 60.0\% \\
        Car        & 70\% & 99.3\% & 99.0\% \\
        Truck      & 25\% & 50.0\% & 80.0\% \\
        Bus        & 20\% & 45.0\% & 78.0\% \\
        Motorcycle  & 10\% & 10.0\% & 55.0\% \\
        Bicycle    & 10\% & 15.0\% & 60.0\% \\
        \midrule
        \textbf{Mean IoU} & \textbf{54.3\%} & \textbf{79.5\%} & \textbf{89.7\%} \\
        \bottomrule
    \end{tabular}
\end{table}

Key observations:
\begin{itemize}
    \item \textbf{Poles} improved from $\sim$12\% $\rightarrow$ 46.5\% $\rightarrow$ 84.0\%, demonstrating the combined effect of boundary weighting and the removal of the RGB bottleneck. Poles are thin structures that are almost entirely composed of boundary pixels.
    \item \textbf{Traffic signs} improved from $\sim$8\% $\rightarrow$ 0\% $\rightarrow$ 82.2\%. The adapter model failed completely on this extremely rare class (only $\sim$12K pixels in the validation set), but the native architecture with Dice loss recovers it effectively.
    \item \textbf{Large classes} (road, building, vegetation, car) maintain $>$94\% IoU across all trained approaches, confirming that the architectural changes do not degrade performance on common classes.
    \item \textbf{Rare dynamic classes} (person, rider, motorcycle, bicycle) show the largest relative gains with the native+Dice configuration, as the Dice loss explicitly counteracts class frequency imbalance.
\end{itemize}

\subsection{Loss Function Ablation}
\label{subsec:loss_ablation}

\paragraph{Boundary Weighting Ablation.}
Table~\ref{tab:boundary_ablation} shows the impact of boundary emphasis $\alpha$ on the adapter model (Phase~2a).

\begin{table}[t]
    \centering
    \caption{Boundary weighting ablation (Adapter model, Phase~2a)}
    \label{tab:boundary_ablation}
    \begin{tabular}{ccccc}
        \toprule
        $\alpha$ & \textbf{mIoU} & \textbf{Pixel Acc.} & \textbf{Pole IoU} & \textbf{Terrain IoU} \\
        \midrule
        0.0 (no boundary) & 64.8\% & 93.8\% & 0.3\% & 24.6\% \\
        8.0 & \textbf{79.5\%} & \textbf{99.3\%} & \textbf{46.5\%} & \textbf{98.0\%} \\
        \midrule
        Improvement & +14.7\% & +5.5\% & +46.2\% & +73.4\% \\
        \bottomrule
    \end{tabular}
\end{table}

Boundary weighting produces a dramatic +14.7\% mIoU improvement. The effect is most pronounced on thin structures (poles: +46.2\%) and classes that frequently border other classes (terrain: +73.4\%).

\paragraph{Dice Loss Ablation.}
Table~\ref{tab:dice_ablation} shows the impact of Dice loss weight $\lambda_{\text{Dice}}$ on the native model (Phase~2b), both configurations using $\alpha=4.0$.

\begin{table}[t]
    \centering
    \caption{Dice loss ablation (Native model, both use $\alpha=4.0$)}
    \label{tab:dice_ablation}
    \begin{tabular}{ccccc}
        \toprule
        $\lambda_{\text{Dice}}$ & \textbf{mIoU} & \textbf{Pixel Acc.} & \textbf{Traffic Sign} & \textbf{Pole} \\
        \midrule
        0.0 (CE only) & 88.0\% & 99.1\% & 78.2\% & 83.7\% \\
        0.5 & \textbf{89.7\%} & 99.0\% & \textbf{82.2\%} & \textbf{84.0\%} \\
        \midrule
        Improvement & +1.7\% & $-$0.1\% & +4.0\% & +0.3\% \\
        \bottomrule
    \end{tabular}
\end{table}

The Dice loss provides a modest but consistent improvement (+1.7\% mIoU) with the most significant gains on rare classes (traffic signs: +4.0\%). There is a minor trade-off: pixel accuracy decreases by 0.1\%, reflecting the inherent tension between class-balanced optimization (Dice) and frequency-weighted optimization (CE).

% -----------------------------------------------------------------
\section{Stage~1 Results: Semantic Video Prediction}
\label{sec:stage1_results}

% TODO: Stage 1 evaluation results to be added once evaluation is complete.
% This section will include quantitative metrics (FID, FVD, mIoU on predicted semantic sequences)
% and qualitative examples of predicted semantic video sequences.

% -----------------------------------------------------------------
\section{Stage~2 Results: Semantic-to-Video Generation}
\label{sec:stage2_results}

This section presents the evaluation results for Stage~2, which generates RGB video frames conditioned on semantic segmentation maps via the ControlNet architecture. The evaluation assesses both the visual quality of generated RGB videos and the degree to which generated frames preserve the semantic structure specified by the conditioning maps. All results are reported on held-out validation clips from the KITTI-360 dataset.

\subsection{Evaluation Protocol}
\label{subsec:eval_protocol}

The Stage~2 evaluation follows a two-step protocol:

\paragraph{Step~1: Generation and Semantic Fidelity.}
A total of 150 validation clips (25 frames each, $192 \times 704$ resolution) are processed through the Stage~2 pipeline. For each clip, the initial RGB frame provides the appearance conditioning (via the RGB VAE and CLIP encoder), while the ground-truth semantic segmentation maps provide the structural conditioning (via the Semantic VAE). The ControlNet-augmented SVD backbone generates RGB video frames using 30 denoising steps with a linearly increasing guidance scale from 1.0 to 3.0.

To evaluate \emph{semantic fidelity}---i.e., how well the generated RGB frames respect the conditioning semantic layout---an independently trained Dilated Residual Network (DRN-D-105)~\cite{yu2017dilated} is applied to segment the generated RGB frames. The predicted segmentation maps are then compared against the ground-truth semantic labels using standard segmentation metrics (mIoU, pixel accuracy, frequency-weighted IoU, mean class accuracy). This protocol measures the degree to which the generated appearance faithfully reflects the intended scene structure, providing a proxy for controllability. Semantic fidelity metrics are computed on the full set of 150 clips (3{,}750 frames).

\paragraph{Step~2: Image and Video Quality Metrics.}
Perceptual and distributional quality metrics are computed between generated and ground-truth RGB frames:
\begin{itemize}
    \item \textbf{Fr\'{e}chet Inception Distance (FID)}~\cite{heusel2017gans}: Measures distributional similarity between generated and real frames using Inception-v3 features (2048-dimensional).
    \item \textbf{Fr\'{e}chet Video Distance (FVD)}~\cite{unterthiner2019fvd}: Extends FID to the temporal domain using I3D features, capturing both appearance quality and temporal consistency.
    \item \textbf{Learned Perceptual Image Patch Similarity (LPIPS)}~\cite{zhang2018unreasonable}: Measures perceptual distance using AlexNet features.
    \item \textbf{Structural Similarity Index (SSIM)}~\cite{wang2004image}: Evaluates structural similarity between generated and ground-truth frames.
    \item \textbf{Peak Signal-to-Noise Ratio (PSNR)}: Measures pixel-level reconstruction accuracy.
\end{itemize}
Due to storage and computational constraints during evaluation, these quality metrics are computed on a subset of 10 videos $\times$ 25 frames = 250 frame pairs.

\subsection{Image and Video Quality}
\label{subsec:image_video_quality}

Table~\ref{tab:video_metrics} summarizes the image and video quality metrics for Stage~2, computed on 10 saved validation videos (250 frame pairs).

\begin{table}[t]
    \centering
    \caption{Stage~2 image and video quality metrics on KITTI-360 validation set (10 videos, 250 frame pairs)}
    \label{tab:video_metrics}
    \begin{tabular}{lc}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        FID $\downarrow$ & \textbf{68.47} \\
        FVD $\downarrow$ & 595.52 \\
        LPIPS $\downarrow$ & 0.334 \\
        SSIM $\uparrow$ & 0.433 $\pm$ 0.137 \\
        PSNR $\uparrow$ (dB) & 14.51 $\pm$ 3.16 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{FID.}
The FID of \textbf{68.47} indicates that the generated frames have realistic appearance and diversity that is reasonably close to the ground-truth distribution within the KITTI-360 domain. This value falls in the good quality range (50--100), confirming that the frozen SVD backbone successfully preserves its learned natural image prior while being steered by the ControlNet conditioning.

\paragraph{FVD.}
The FVD of 595.52 reflects moderate temporal consistency. This metric captures both per-frame quality and inter-frame coherence via I3D video features. The higher value suggests some temporal variation between consecutive frames, which is expected for diffusion-based generation where each frame is denoised within a shared temporal attention framework. Temporal coherence is primarily governed by the temporal transformer blocks inherited from SVD, and the ControlNet contributes spatial conditioning without explicit temporal smoothing.

\paragraph{Perceptual and Pixel-Level Metrics.}
The LPIPS of 0.334 indicates that generated frames are perceptually recognizable as depicting the same scene as the ground truth, with notable differences in fine details such as textures and lighting. The moderate SSIM (0.433) and PSNR (14.51~dB) are expected for generative models: unlike reconstruction-based approaches, diffusion models produce \emph{plausible} outputs rather than pixel-identical copies. These metrics are therefore less informative for evaluating generative quality than distributional measures (FID) and semantic fidelity metrics.

Table~\ref{tab:per_video_quality} provides a per-video breakdown of structural and pixel-level metrics, illustrating the variation across different driving scenes.

\begin{table}[t]
    \centering
    \caption{Per-video SSIM and PSNR for the 10 saved Stage~2 validation clips}
    \label{tab:per_video_quality}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Video} & \textbf{SSIM} & \textbf{PSNR (dB)} \\
        \midrule
        Clip~0 & 0.375 & 14.18 \\
        Clip~1 & 0.562 & 16.12 \\
        Clip~2 & 0.522 & 15.68 \\
        Clip~3 & 0.501 & 14.79 \\
        Clip~4 & 0.424 & 15.39 \\
        Clip~5 & 0.358 & 12.61 \\
        Clip~6 & 0.461 & 14.63 \\
        Clip~7 & 0.490 & 15.58 \\
        Clip~8 & 0.303 & 11.90 \\
        Clip~9 & 0.337 & 14.23 \\
        \midrule
        \textbf{Average} & \textbf{0.433} & \textbf{14.51} \\
        \bottomrule
    \end{tabular}
\end{table}

The per-video variation is considerable (SSIM range: 0.303--0.562), reflecting the diversity of scene complexity in the validation set. Clips with relatively static, well-structured scenes (Clip~1, Clip~2) achieve higher similarity scores, while clips featuring complex dynamics or unusual viewpoints (Clip~5, Clip~8) exhibit lower similarity to the ground truth.

\subsection{Semantic Fidelity}
\label{subsec:semantic_fidelity}

To evaluate whether the generated RGB frames faithfully encode the semantic structure specified by the conditioning maps, we apply a pretrained DRN-D-105 segmentation network~\cite{yu2017dilated} (trained on KITTI-360 with 19 classes) to segment the generated RGB frames and compare the predicted segmentation against the ground-truth labels. This evaluation is conducted on the full set of 150 validation clips $\times$ 25 frames = 3{,}750 generated frames, providing a robust assessment of semantic fidelity across diverse driving scenarios.

\paragraph{Overall Metrics.}
Table~\ref{tab:semantic_fidelity} presents the overall semantic fidelity metrics.

\begin{table}[t]
    \centering
    \caption{Semantic fidelity of Stage~2 generated RGB frames (150 clips, 3{,}750 frames), measured by applying DRN-D-105 segmentation to generated frames and comparing against ground-truth labels}
    \label{tab:semantic_fidelity}
    \begin{tabular}{lc}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        mIoU & 45.60\% \\
        Overall Pixel Accuracy & 89.19\% \\
        Mean Class Accuracy & 60.87\% \\
        Frequency-Weighted IoU & 81.01\% \\
        \bottomrule
    \end{tabular}
\end{table}

The mIoU of 45.60\% indicates that the generated RGB frames contain sufficient visual detail for an external segmentation network to recover a substantial portion of the intended semantic layout. The high pixel accuracy (89.19\%) and frequency-weighted IoU (81.01\%) demonstrate that large, spatially dominant classes are generated with high fidelity. The gap between pixel accuracy and mIoU reflects the challenge of correctly generating rare or small-scale classes, which contribute equally to the mIoU computation despite occupying a small fraction of image pixels.

\paragraph{Per-Class Analysis.}
Table~\ref{tab:stage2_per_class} presents the per-class IoU across all 19 semantic categories, revealing a clear pattern related to class frequency and spatial extent.

\begin{table}[t]
    \centering
    \caption{Per-class IoU for Stage~2 semantic fidelity evaluation (150 clips). Classes are grouped by generation quality tier}
    \label{tab:stage2_per_class}
    \begin{tabular}{lc}
        \toprule
        \textbf{Class} & \textbf{IoU (\%)} \\
        \midrule
        \multicolumn{2}{l}{\textit{Well-generated (IoU $>$ 70\%)}} \\
        Road         & \textbf{90.09} \\
        Car          & \textbf{88.75} \\
        Sky          & \textbf{87.24} \\
        Building     & \textbf{84.25} \\
        Vegetation   & \textbf{82.35} \\
        \midrule
        \multicolumn{2}{l}{\textit{Moderately generated (IoU 29--70\%)}} \\
        Terrain      & 69.78 \\
        Sidewalk     & 61.13 \\
        Truck        & 55.25 \\
        Fence        & 48.10 \\
        Wall         & 45.35 \\
        Pole         & 37.84 \\
        Traffic Sign & 34.65 \\
        Person       & 29.61 \\
        Motorcycle   & 29.27 \\
        \midrule
        \multicolumn{2}{l}{\textit{Poorly generated (IoU $<$ 20\%)}} \\
        Rider        & 19.01 \\
        Bicycle      & 3.66 \\
        Traffic Light & 0.00 \\
        Bus          & 0.00 \\
        Train        & 0.00 \\
        \midrule
        \textbf{Mean IoU} & \textbf{45.60} \\
        \bottomrule
    \end{tabular}
\end{table}

The results reveal a clear tiered structure:

\begin{itemize}
    \item \textbf{Well-generated classes (IoU $>$ 70\%):} Road (90.09\%), car (88.75\%), sky (87.24\%), building (84.25\%), and vegetation (82.35\%). These five spatially dominant classes with large contiguous regions and clear visual patterns are generated with high fidelity, confirming that the ControlNet successfully translates large-scale semantic structure into photorealistic appearance.

    \item \textbf{Moderately generated classes (IoU 29--70\%):} This tier spans a wide range, from terrain (69.78\%) and sidewalk (61.13\%) down to person (29.61\%) and motorcycle (29.27\%). These classes are either less frequent, smaller in spatial extent, or have more complex boundary structures. Notably, truck achieves 55.25\% IoU, demonstrating the model's ability to render large vehicle classes with reasonable fidelity. Pole (37.84\%) and traffic sign (34.65\%) reflect the difficulty of generating thin or small structures.

    \item \textbf{Poorly generated classes (IoU $<$ 20\%):} Rider (19.01\%), bicycle (3.66\%), traffic light (0.00\%), bus (0.00\%), and train (0.00\%). Traffic light, bus, and train are absent from the evaluation clips, making their 0\% IoU uninformative rather than indicative of model failure. The low IoU for rider and bicycle reflects both their rarity in the training data and their fine-grained visual detail, which is challenging to reproduce at $192 \times 704$ resolution.
\end{itemize}

\paragraph{Confusion Matrix Analysis.}
Figure~\ref{fig:confusion_matrix} presents the $19 \times 19$ confusion matrix obtained by applying DRN-D-105 to the generated frames. The matrix reveals several systematic confusion patterns that provide insight into the generation characteristics of the model.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{images/stage2_confusion_matrix.png}
    \caption{Confusion matrix for DRN-D-105 segmentation applied to Stage~2 generated RGB frames (150 clips, 3{,}750 frames). Rows correspond to ground-truth classes and columns to DRN predictions. The strong diagonal confirms high semantic fidelity for dominant classes (road, building, vegetation, sky, car). Off-diagonal entries reveal systematic confusions: sidewalk$\leftrightarrow$road, pole$\leftrightarrow$building, and rider$\leftrightarrow$person.}
    \label{fig:confusion_matrix}
\end{figure}

The strong diagonal entries for road, building, vegetation, sky, and car confirm that these classes are generated with high visual fidelity---the DRN network confidently and correctly segments them. Several notable off-diagonal patterns emerge:

\begin{itemize}
    \item \textbf{Sidewalk $\leftrightarrow$ road:} A portion of sidewalk pixels are predicted as road and vice versa. These classes share similar surface textures and are spatially adjacent, making boundary-precise generation particularly challenging.

    \item \textbf{Pole $\leftrightarrow$ building:} Thin pole structures are frequently confused with building pixels. This reflects the difficulty of generating fine vertical structures at the operating resolution, where poles may span only a few pixels in width.

    \item \textbf{Rider $\leftrightarrow$ person:} The rider class exhibits confusion with person, as these classes share similar visual appearance and differ primarily in the presence of an associated vehicle (bicycle or motorcycle).

    \item \textbf{Fence $\leftrightarrow$ vegetation:} Fence pixels show notable confusion with vegetation, consistent with their frequent spatial co-occurrence in suburban driving scenes where fences border vegetated areas.

    \item \textbf{Traffic sign $\leftrightarrow$ pole/vegetation:} Traffic signs are partially confused with surrounding pole and vegetation pixels, reflecting the challenge of generating small elevated objects against complex backgrounds.
\end{itemize}

These confusion patterns are largely expected given the visual similarity between the confused classes and the spatial resolution constraints. Importantly, they represent limitations of generating fine-grained details rather than fundamental failures in semantic conditioning.

\subsection{Qualitative Results}
\label{subsec:qualitative}

Figure~\ref{fig:stage2_qualitative} presents representative qualitative comparisons between generated and ground-truth frames. Each panel shows the ground-truth RGB frame (top-left), the generated RGB frame (top-right), the ground-truth semantic map colorized for visualization (bottom-left), and the DRN-predicted segmentation of the generated frame (bottom-right).

\begin{figure}[t]
    \centering
    \subfloat[Clip~0, Frame~5: Suburban street scene with parked vehicles\label{fig:stage2_qual_v0}]{%
        \includegraphics[width=0.95\textwidth]{images/stage2_comparison_v0_f5.png}}\\[0.3cm]
    \subfloat[Clip~1, Frame~5: Residential area with vegetation and buildings\label{fig:stage2_qual_v1}]{%
        \includegraphics[width=0.95\textwidth]{images/stage2_comparison_v1_f5.png}}\\[0.3cm]
    \subfloat[Clip~3, Frame~5: Road scene with diverse semantic elements\label{fig:stage2_qual_v3}]{%
        \includegraphics[width=0.95\textwidth]{images/stage2_comparison_v3_f5.png}}
    \caption{Qualitative Stage~2 results. Each panel: ground-truth RGB (top-left), generated RGB (top-right), ground-truth semantic map (bottom-left), DRN segmentation of the generated frame (bottom-right). The generated frames reproduce the overall scene layout, road geometry, and major object positions. Differences are most visible in fine-grained details such as foliage textures and building fa\c{c}ades.}
    \label{fig:stage2_qualitative}
\end{figure}

Several observations emerge from the qualitative analysis:

\begin{itemize}
    \item \textbf{Global layout preservation:} The generated frames consistently reproduce the overall spatial arrangement of roads, buildings, vegetation, and sky regions. The ControlNet conditioning effectively translates the semantic map into a spatially coherent appearance.

    \item \textbf{Object placement:} Vehicles, pedestrians, and other foreground objects appear at locations consistent with the conditioning semantic map. The car class is particularly well-reproduced, aligning with its high per-class IoU of 88.75\%.

    \item \textbf{Appearance plausibility:} The frozen SVD backbone contributes realistic textures, lighting, and color distributions. Generated frames exhibit natural-looking road surfaces, building facades, and vegetation, even though the specific pixel-level details differ from the ground truth.

    \item \textbf{Fine detail limitations:} Differences between generated and ground-truth frames are concentrated in fine-grained features: tree branch patterns, window details, and object edges. This is consistent with the moderate SSIM and PSNR scores, which penalize pixel-level deviations.

    \item \textbf{Semantic consistency:} The DRN segmentation of generated frames (bottom-right) closely matches the ground-truth semantic map (bottom-left), confirming that the generated appearance encodes the correct semantic content. Minor discrepancies at class boundaries are visible, consistent with the gap between pixel accuracy (89.19\%) and mIoU (45.60\%).
\end{itemize}
