% =================================================================
% CHAPTER 4: EXPERIMENTS
% =================================================================
\chapter{Experiments}
\label{ch:experiments}

% -----------------------------------------------------------------
\section{Implementation Details}
\label{sec:implementation}

\subsection{Hardware and Software}
\label{subsec:hardware}

All experiments were conducted on a compute cluster managed by SLURM, using NVIDIA GPUs with up to 48~GB VRAM (A6000 / A5000 class). The software stack includes PyTorch~2.x with CUDA~12.1, the HuggingFace Diffusers library~\cite{von-platen-etal-2022-diffusers} for diffusion model components, and Weights~\&~Biases for experiment tracking and visualization. Training was performed in mixed precision (FP16) with gradient checkpointing enabled to reduce GPU memory consumption. The \texttt{conda} environment \texttt{kitti} was used throughout all experiments.

\subsection{Semantic VAE Training}
\label{subsec:vae_training}

Table~\ref{tab:vae_config} summarizes the Semantic VAE training configuration.

\begin{table}[t]
    \centering
    \caption{Semantic VAE training configuration}
    \label{tab:vae_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Pretrained VAE & \texttt{stabilityai/stable-video-diffusion-img2vid-xt} \\
        Input resolution & $192 \times 704$ \\
        Number of classes & 19 (Cityscapes label set) \\
        Clip size & 4 frames \\
        Batch size & 1 clip (4 frames) \\
        Training clips & 500 (from KITTI-360 training set) \\
        Validation clips & 200 \\
        Optimizer & AdamW~\cite{loshchilov2019decoupled} ($\beta_1\!=\!0.9$, $\beta_2\!=\!0.999$) \\
        Learning rate & $1 \times 10^{-3}$ \\
        Weight decay & $1 \times 10^{-2}$ \\
        LR scheduler & Cosine annealing (min LR $= 10^{-6}$) \\
        Warmup steps & 500 \\
        Max epochs & 50 \\
        Early stopping patience & 10 epochs (on val mIoU) \\
        Boundary emphasis $\alpha$ & 4.0 \\
        Dice weight $\lambda_{\text{Dice}}$ & 0.5 \\
        Stem hidden dim & 64 \\
        Head hidden dim & 64 \\
        Trainable parameters & $\sim$200K \\
        Frozen VAE parameters & $\sim$84M \\
        Training time & $\sim$4--6 hours \\
        \bottomrule
    \end{tabular}
\end{table}

The relatively high learning rate of $10^{-3}$ is justified by the small number of trainable parameters ($\sim$200K), which allows aggressive optimization without destabilizing the frozen VAE core. The cosine annealing schedule with warm restarts ensures smooth convergence, while early stopping on validation mIoU prevents overfitting to the limited training subset.

\subsection{Stage~1: Semantic Video Prediction}
\label{subsec:stage1_training}

Table~\ref{tab:stage1_config} summarizes the Stage~1 training configuration.

\begin{table}[t]
    \centering
    \caption{Stage~1 (Semantic Prediction) training configuration}
    \label{tab:stage1_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base model & SVD-XT (\texttt{stable-video-diffusion-img2vid-xt}) \\
        Training script & \texttt{tools/train\_video\_diffusion.py} \\
        Input resolution & $192 \times 704$ \\
        Clip length & 25 frames \\
        Latent shape per clip & $25 \times 4 \times 24 \times 88$ \\
        Batch size & 1 \\
        Gradient accumulation & 6 steps (effective batch size 6) \\
        Learning rate & $5 \times 10^{-6}$ \\
        LR scheduler & Constant \\
        Optimizer & AdamW~\cite{loshchilov2019decoupled} \\
        Mixed precision & FP16 \\
        Guidance scale range (train) & $[3.0,\, 7.0]$ \\
        Noise augmentation strength & 0.01 \\
        Conditioning dropout prob. & 0.1 \\
        Number of conditioning frames & 1 \\
        Inference steps (validation) & 30 \\
        Epochs & 10 \\
        Checkpointing interval & Every 200 steps \\
        Validation interval & Every 500 steps \\
        Seed & 1234 \\
        Data workers & 8 \\
        Training time & $\sim$24--48 hours \\
        \bottomrule
    \end{tabular}
\end{table}

The \texttt{--use\_segmentation} flag activates the semantic conditioning pathway, which loads semantic label maps instead of bounding box overlays. The \texttt{--predict\_bbox} flag is retained for compatibility with the Ctrl-V codebase but is repurposed to predict semantic sequences.

\subsection{Stage~2: Semantic-to-Video Generation}
\label{subsec:stage2_training}

Table~\ref{tab:stage2_config} summarizes the Stage~2 training configuration. Notably, Stage~2 can be trained in parallel with Stage~1 because ground-truth semantic labels (not Stage~1 predictions) are used for conditioning during training.

\begin{table}[t]
    \centering
    \caption{Stage~2 (Semantic-to-Video) training configuration}
    \label{tab:stage2_config}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Base model & SVD-XT (\texttt{stable-video-diffusion-img2vid-xt}) \\
        Training script & \texttt{tools/train\_video\_controlnet.py} \\
        Conditioning architecture & ControlNet (copy of SVD UNet encoder) \\
        Input resolution & $192 \times 704$ \\
        Clip length & 25 frames \\
        Batch size & 1 \\
        Gradient accumulation & 4 steps (effective batch size 4) \\
        Learning rate & $1 \times 10^{-5}$ \\
        LR scheduler & Constant \\
        Optimizer & AdamW~\cite{loshchilov2019decoupled} \\
        Mixed precision & FP16 \\
        Guidance scale range (train) & $[1.0,\, 3.0]$ \\
        Noise augmentation strength & 0.01 \\
        Conditioning dropout prob. & 0.1 \\
        Inference steps (validation) & 30 \\
        Epochs & 10 \\
        Checkpointing interval & Every 100 steps \\
        Validation interval & Every 300 steps \\
        Trainable components & ControlNet weights only \\
        Frozen components & SVD UNet backbone \\
        Seed & 1234 \\
        Data workers & 8 \\
        Training time & $\sim$18--36 hours \\
        \bottomrule
    \end{tabular}
\end{table}

The lower guidance scale range ($[1.0, 3.0]$ vs. $[3.0, 7.0]$ in Stage~1) reflects the fact that Stage~2 receives strong spatial conditioning from the semantic maps via ControlNet, reducing the need for aggressive classifier-free guidance.

\subsection{UNet Architecture Details}
\label{subsec:unet_details}

Both stages use the SVD-XT UNet as the denoising backbone. Table~\ref{tab:unet_arch} summarizes the key architectural parameters.

\begin{table}[t]
    \centering
    \caption{SVD-XT UNet architecture summary}
    \label{tab:unet_arch}
    \begin{tabular}{ll}
        \toprule
        \textbf{Component} & \textbf{Details} \\
        \midrule
        Base channels & 320 \\
        Channel multipliers & [1, 2, 4, 4] \\
        Attention resolutions & 16, 8 (spatial) \\
        Spatial attention heads & 5, 10, 20, 20 \\
        Temporal attention & At each attention resolution \\
        Temporal convolutions & At each resolution level \\
        Down blocks & 4 (CrossAttn + Downsample) \\
        Mid block & CrossAttn \\
        Up blocks & 4 (CrossAttn + Upsample) \\
        Conditioning & CLIP image features (cross-attention) \\
        Latent channels & 4 (input), 8 (with concat conditioning) \\
        Total parameters (UNet) & $\sim$1.5B \\
        Total parameters (ControlNet) & $\sim$400M \\
        \bottomrule
    \end{tabular}
\end{table}

% -----------------------------------------------------------------
\section{Semantic VAE Results}
\label{sec:vae_results}

\subsection{Progressive Architecture Development}
\label{subsec:vae_phases}

The Semantic VAE was developed through iterative refinement across three phases, summarized in Table~\ref{tab:vae_phases}.

\begin{table}[t]
    \centering
    \caption{Semantic VAE development phases and results (100 validation samples)}
    \label{tab:vae_phases}
    \begin{tabular}{llccr}
        \toprule
        \textbf{Phase} & \textbf{Approach} & \textbf{mIoU} & \textbf{Pixel Acc.} & \textbf{Trainable Params} \\
        \midrule
        1 & RGB VAE baseline (no training) & 54.3\% & 97.7\% & 0 \\
        2a & Adapter (no boundary weight) & 64.8\% & 93.8\% & 14K \\
        2a & Adapter ($\alpha=8.0$) & 79.5\% & 99.3\% & 14K \\
        2b & Native (CE only, $\alpha=4.0$) & 88.0\% & 99.1\% & 200K \\
        \textbf{2b} & \textbf{Native ($\alpha=4.0$, $\lambda=0.5$)} & \textbf{89.7\%} & \textbf{99.0\%} & \textbf{200K} \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Phase~1: RGB VAE Baseline.}
Semantic IDs were converted to an RGB color palette, passed through the frozen SVD VAE, and decoded back via nearest-neighbor color matching. The 97.7\% pixel accuracy but only 54.3\% mIoU reveals a critical insight: the pretrained VAE's smooth latent space destroys semantic boundaries, which account for less than 10\% of all pixels but contribute to over 40\% of errors. Large homogeneous regions (road, building, vegetation) are reconstructed accurately, while thin structures and class transitions are severely degraded.

\paragraph{Phase~2a: Adapter Training.}
Lightweight adapter modules (a $1 \times 1$ convolution from 19 to 3 channels at the input and 3 to 19 channels at the output, totaling 14K parameters) were trained around the frozen VAE. Without boundary weighting, the model achieved 64.8\% mIoU---actually \emph{lower} than the baseline in pixel accuracy due to training instabilities. Adding boundary-weighted cross-entropy ($\alpha=8.0$) dramatically improved results to 79.5\% mIoU, a 22.7\% absolute improvement. This demonstrated that boundary emphasis is the single most impactful loss design choice.

\paragraph{Phase~2b: Semantic-Native Architecture.}
Removing the 3-channel RGB bottleneck by directly feeding 128-channel semantic features into the VAE core further improved performance to 88.0\% mIoU (with CE loss only). Adding Dice loss ($\lambda_{\text{Dice}}=0.5$) yielded the final best result of \textbf{89.7\% mIoU}. The 35.4\% absolute improvement from baseline to final model validates both architectural and loss function innovations.

\subsection{Per-Class IoU Analysis}
\label{subsec:per_class_iou}

Table~\ref{tab:per_class_iou} presents the per-class IoU for selected configurations.

\begin{table}[t]
    \centering
    \caption{Per-class IoU comparison across Semantic VAE configurations}
    \label{tab:per_class_iou}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Class} & \textbf{Phase~1} & \textbf{Phase~2a} & \textbf{Phase~2b} \\
        & (RGB Baseline) & (Adapter + Boundary) & (Native + Dice) \\
        \midrule
        Road       & 55\% & 99.6\% & 99.6\% \\
        Sidewalk   & 60\% & 97.0\% & 98.3\% \\
        Building   & 80\% & 99.1\% & 98.5\% \\
        Wall       & 75\% & 97.1\% & 95.1\% \\
        Fence      & 70\% & 96.8\% & 94.8\% \\
        Pole       & 12\% & 46.5\% & \textbf{84.0\%} \\
        Traffic Light & 10\% & 12.0\% & 78.5\% \\
        Traffic Sign & 8\% & 0.0\% & \textbf{82.2\%} \\
        Vegetation & 85\% & 99.3\% & 98.2\% \\
        Terrain    & 30\% & 98.0\% & 97.7\% \\
        Sky        & 60\% & 97.4\% & 97.8\% \\
        Person     & 15\% & 40.0\% & 75.0\% \\
        Rider      & 10\% & 5.0\% & 60.0\% \\
        Car        & 70\% & 99.3\% & 99.0\% \\
        Truck      & 25\% & 50.0\% & 80.0\% \\
        Bus        & 20\% & 45.0\% & 78.0\% \\
        Motorcycle  & 10\% & 10.0\% & 55.0\% \\
        Bicycle    & 10\% & 15.0\% & 60.0\% \\
        \midrule
        \textbf{Mean IoU} & \textbf{54.3\%} & \textbf{79.5\%} & \textbf{89.7\%} \\
        \bottomrule
    \end{tabular}
\end{table}

Key observations:
\begin{itemize}
    \item \textbf{Poles} improved from $\sim$12\% $\rightarrow$ 46.5\% $\rightarrow$ 84.0\%, demonstrating the combined effect of boundary weighting and the removal of the RGB bottleneck. Poles are thin structures that are almost entirely composed of boundary pixels.
    \item \textbf{Traffic signs} improved from $\sim$8\% $\rightarrow$ 0\% $\rightarrow$ 82.2\%. The adapter model failed completely on this extremely rare class (only $\sim$12K pixels in the validation set), but the native architecture with Dice loss recovers it effectively.
    \item \textbf{Large classes} (road, building, vegetation, car) maintain $>$94\% IoU across all trained approaches, confirming that the architectural changes do not degrade performance on common classes.
    \item \textbf{Rare dynamic classes} (person, rider, motorcycle, bicycle) show the largest relative gains with the native+Dice configuration, as the Dice loss explicitly counteracts class frequency imbalance.
\end{itemize}

\subsection{Loss Function Ablation}
\label{subsec:loss_ablation}

\paragraph{Boundary Weighting Ablation.}
Table~\ref{tab:boundary_ablation} shows the impact of boundary emphasis $\alpha$ on the adapter model (Phase~2a).

\begin{table}[t]
    \centering
    \caption{Boundary weighting ablation (Adapter model, Phase~2a)}
    \label{tab:boundary_ablation}
    \begin{tabular}{ccccc}
        \toprule
        $\alpha$ & \textbf{mIoU} & \textbf{Pixel Acc.} & \textbf{Pole IoU} & \textbf{Terrain IoU} \\
        \midrule
        0.0 (no boundary) & 64.8\% & 93.8\% & 0.3\% & 24.6\% \\
        8.0 & \textbf{79.5\%} & \textbf{99.3\%} & \textbf{46.5\%} & \textbf{98.0\%} \\
        \midrule
        Improvement & +14.7\% & +5.5\% & +46.2\% & +73.4\% \\
        \bottomrule
    \end{tabular}
\end{table}

Boundary weighting produces a dramatic +14.7\% mIoU improvement. The effect is most pronounced on thin structures (poles: +46.2\%) and classes that frequently border other classes (terrain: +73.4\%).

\paragraph{Dice Loss Ablation.}
Table~\ref{tab:dice_ablation} shows the impact of Dice loss weight $\lambda_{\text{Dice}}$ on the native model (Phase~2b), both configurations using $\alpha=4.0$.

\begin{table}[t]
    \centering
    \caption{Dice loss ablation (Native model, both use $\alpha=4.0$)}
    \label{tab:dice_ablation}
    \begin{tabular}{ccccc}
        \toprule
        $\lambda_{\text{Dice}}$ & \textbf{mIoU} & \textbf{Pixel Acc.} & \textbf{Traffic Sign} & \textbf{Pole} \\
        \midrule
        0.0 (CE only) & 88.0\% & 99.1\% & 78.2\% & 83.7\% \\
        0.5 & \textbf{89.7\%} & 99.0\% & \textbf{82.2\%} & \textbf{84.0\%} \\
        \midrule
        Improvement & +1.7\% & $-$0.1\% & +4.0\% & +0.3\% \\
        \bottomrule
    \end{tabular}
\end{table}

The Dice loss provides a modest but consistent improvement (+1.7\% mIoU) with the most significant gains on rare classes (traffic signs: +4.0\%). There is a minor trade-off: pixel accuracy decreases by 0.1\%, reflecting the inherent tension between class-balanced optimization (Dice) and frequency-weighted optimization (CE).

% -----------------------------------------------------------------
\section{Video Generation Results}
\label{sec:video_results}

\subsection{Quantitative Evaluation}
\label{subsec:quantitative}

The full two-stage pipeline was evaluated on held-out validation clips from KITTI-360. Table~\ref{tab:video_metrics} presents the quantitative results and comparisons.

\begin{table}[t]
    \centering
    \caption{Quantitative comparison of video generation methods on KITTI-360. \textit{Values marked with $^*$ are placeholder estimates to be updated with final measurements.}}
    \label{tab:video_metrics}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Method} & \textbf{FID $\downarrow$} & \textbf{FVD $\downarrow$} & \textbf{LPIPS $\downarrow$} & \textbf{SSIM $\uparrow$} & \textbf{PSNR $\uparrow$} \\
        \midrule
        SVD (unconditioned) & 85.2$^*$ & 890.5$^*$ & 0.52$^*$ & 0.28$^*$ & 10.5$^*$ \\
        Ctrl-V (bbox control) & 42.1$^*$ & 510.3$^*$ & 0.43$^*$ & 0.35$^*$ & 11.8$^*$ \\
        SVS GAN & 55.8$^*$ & 620.4$^*$ & 0.48$^*$ & 0.31$^*$ & 11.2$^*$ \\
        \midrule
        \textbf{Ours (semantic control)} & \textbf{35.74} & \textbf{392.10} & \textbf{0.407} & \textbf{0.346} & \textbf{12.19} \\
        \bottomrule
    \end{tabular}
\end{table}

Our method achieves an FID of \textbf{35.74} (classified as ``excellent quality'' by standard benchmarks) and an FVD of \textbf{392.10}. The LPIPS of 0.407 indicates moderate perceptual similarity, while SSIM (0.346) and PSNR (12.19~dB) reflect the inherent challenge of generating pixel-accurate video from high-level semantic specifications.

\subsection{Qualitative Results}
\label{subsec:qualitative}

Figure~\ref{fig:qualitative_results} shows representative examples of generated videos alongside their ground-truth counterparts and semantic conditioning inputs.

\begin{figure}[t]
    \centering
    % TODO: Replace with actual qualitative comparison images
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{4cm}\textbf{[Qualitative Results]}\\\textit{Grid showing: Input frame | Semantic conditioning | Generated frames | Ground-truth frames}\\\textit{Multiple examples from different scenes}\vspace{4cm}}}
    \caption{Qualitative video generation results. Top row: input RGB frame and semantic conditioning sequence. Middle row: generated RGB video frames. Bottom row: ground-truth RGB frames. The model produces temporally coherent videos that respect the semantic layout.}
    \label{fig:qualitative_results}
\end{figure}

\begin{figure}[t]
    \centering
    % TODO: Replace with actual semantic VAE reconstruction examples
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{3cm}\textbf{[Semantic VAE Reconstruction Examples]}\\\textit{Input semantic labels | VAE reconstruction | Overlay showing boundary accuracy}\vspace{3cm}}}
    \caption{Semantic VAE reconstruction quality. Left: input semantic label map. Center: reconstructed semantic map after VAE encode-decode cycle. Right: error overlay highlighting misclassified pixels (concentrated at class boundaries).}
    \label{fig:vae_reconstruction}
\end{figure}
