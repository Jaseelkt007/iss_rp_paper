% =================================================================
% CHAPTER 2: RELATED WORK
% =================================================================
\chapter{Related Work}
\label{ch:related_work}

This chapter provides the theoretical background and reviews the state of the art in the areas relevant to this thesis: diffusion models, variational autoencoders, video generation, and controllable synthesis.

% -----------------------------------------------------------------
\section{Continuous-Time Generative Modeling: From Flows to Diffusion}
\label{sec:diffusion_models}

Modern generative models such as diffusion models and flow matching methods can be understood from a unified perspective: they construct a generative process by simulating a differential equation that transforms a simple distribution into a complex data distribution. This section presents a continuous-time formulation based on ordinary and stochastic differential equations, providing a principled foundation for both flow models and diffusion models.

\subsection{Generative Modeling as Distribution Transport}
\label{subsec:distribution_transport}

Let $p_{\text{data}}(\mathbf{x})$ denote the data distribution over $\mathbb{R}^d$, and let $p_0(\mathbf{x})$ denote a simple prior distribution, typically chosen as a standard Gaussian:
\begin{align}
    p_0(\mathbf{x}) = \mathcal{N}(\mathbf{0}, \mathbf{I}).
    \label{eq:prior}
\end{align}
The objective of generative modeling is to construct a transformation that maps samples from $p_0$ to samples from $p_{\text{data}}$.

A natural way to formalize this transformation is to define a \emph{probability path}
\begin{align}
    \{p_t(\mathbf{x})\}_{t \in [0,1]}
    \label{eq:prob_path}
\end{align}
such that
\begin{align}
    p_0(\mathbf{x}) = \mathcal{N}(\mathbf{0}, \mathbf{I}), \qquad p_1(\mathbf{x}) = p_{\text{data}}(\mathbf{x}).
    \label{eq:path_boundary}
\end{align}
If we can construct dynamics that evolve samples according to this path, we obtain a continuous-time generative model. The two main approaches to constructing such dynamics---ordinary differential equations and stochastic differential equations---form the mathematical foundation of modern generative modeling~\cite{song2021scorebased, lipman2023flow, chen2018neural}.

\subsection{Deterministic Flows and Ordinary Differential Equations}
\label{subsec:ode_flows}

We first consider deterministic dynamics defined by an Ordinary Differential Equation (ODE):
\begin{align}
    \frac{d\mathbf{x}_t}{dt} = \mathbf{v}(\mathbf{x}_t, t),
    \label{eq:flow_ode}
\end{align}
where $\mathbf{x}_t \in \mathbb{R}^d$ denotes the state at time $t$, and $\mathbf{v}(\mathbf{x}, t)$ is a time-dependent \emph{vector field} that assigns an instantaneous velocity to every point in space and time. Given an initial sample $\mathbf{x}_0 \sim p_0$, solving the ODE forward in time transports it along a trajectory toward the data distribution.

\paragraph{Evolution of the Probability Density.}
If samples evolve according to the ODE above, the corresponding probability density $p_t(\mathbf{x})$ satisfies the \emph{continuity equation}:
\begin{align}
    \frac{\partial p_t(\mathbf{x})}{\partial t} = -\nabla \cdot \bigl(p_t(\mathbf{x})\,\mathbf{v}(\mathbf{x}, t)\bigr).
    \label{eq:continuity}
\end{align}
This equation expresses conservation of probability mass: the divergence term $\nabla \cdot \mathbf{v}$ controls local expansion or contraction of probability density. Thus, learning a generative model reduces to learning a vector field $\mathbf{v}$ that induces the desired density evolution from $p_0$ to $p_{\text{data}}$.

Neural ODEs~\cite{chen2018neural} parameterize $\mathbf{v}$ with a neural network and solve Equation~\eqref{eq:flow_ode} using numerical ODE solvers, providing a flexible framework for continuous-time generative modeling.

\subsection{Conditional Probability Paths}
\label{subsec:conditional_paths}

Instead of specifying $\mathbf{v}$ directly, it is often more convenient to first define a \emph{conditional probability path}. The marginal path is obtained by averaging over data samples:
\begin{align}
    p_t(\mathbf{x}) = \int p_t(\mathbf{x} \mid \mathbf{x}_1)\, p_{\text{data}}(\mathbf{x}_1)\, d\mathbf{x}_1,
    \label{eq:marginal_path}
\end{align}
where $\mathbf{x}_1 \sim p_{\text{data}}$ is a clean data sample and $p_t(\mathbf{x} \mid \mathbf{x}_1)$ is a designed interpolation between noise and data. A common choice is a \emph{Gaussian conditional path}:
\begin{align}
    \mathbf{x}_t = \alpha(t)\,\mathbf{x}_1 + \sigma(t)\,\boldsymbol{\epsilon}, \qquad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
    \label{eq:gaussian_interpolant}
\end{align}
where $\alpha(t)$ and $\sigma(t)$ are smooth scalar functions satisfying the boundary conditions:
\begin{align}
    \alpha(0) = 0,\quad \sigma(0) = 1 \qquad \text{(pure noise at } t=0\text{)}, \notag\\
    \alpha(1) = 1,\quad \sigma(1) = 0 \qquad \text{(clean data at } t=1\text{)}.
    \label{eq:path_conditions}
\end{align}
This construction explicitly defines how data points are gradually corrupted into noise (or equivalently, how noise is gradually shaped into data). Different choices of $\alpha(t)$ and $\sigma(t)$ recover different generative frameworks, as discussed in Section~\ref{subsec:unified_view}.

\subsection{Conditional and Marginal Vector Fields}
\label{subsec:vector_fields}

For a fixed data point $\mathbf{x}_1$, the conditional path of Equation~\eqref{eq:gaussian_interpolant} defines a trajectory in expectation. The corresponding \emph{conditional vector field} is obtained by differentiating:
\begin{align}
    \mathbf{v}^*(\mathbf{x}_t, t \mid \mathbf{x}_1) = \frac{d}{dt}\,\mathbb{E}[\mathbf{x}_t \mid \mathbf{x}_1] = \dot{\alpha}(t)\,\mathbf{x}_1 + \dot{\sigma}(t)\,\boldsymbol{\epsilon},
    \label{eq:cond_vector_field}
\end{align}
where $\dot{\alpha}$ and $\dot{\sigma}$ denote time derivatives. However, during generation we do not know $\mathbf{x}_1$; we only observe $\mathbf{x}_t$. Therefore, the true vector field governing the marginal distribution is obtained by taking the conditional expectation:
\begin{align}
    \mathbf{v}^*(\mathbf{x}_t, t) = \mathbb{E}\!\left[\mathbf{v}^*(\mathbf{x}_t, t \mid \mathbf{x}_1) \;\middle|\; \mathbf{x}_t\right].
    \label{eq:marginal_vector_field}
\end{align}
This \emph{marginal vector field} uniquely determines the ODE that transports the full probability distribution from $p_0$ to $p_{\text{data}}$~\cite{lipman2023flow}.

\subsection{Flow Matching Objective}
\label{subsec:flow_matching}

Since the true marginal vector field $\mathbf{v}^*(\mathbf{x}, t)$ is intractable in closed form, we approximate it with a neural network $\mathbf{v}_\theta(\mathbf{x}, t)$. Lipman et al.~\cite{lipman2023flow} showed that the \emph{flow matching} objective can be formulated using only the conditional vector field:
\begin{align}
    \mathcal{L}_{\text{FM}} = \mathbb{E}_{t,\, \mathbf{x}_1,\, \mathbf{x}_t}\!\left[\left\|\mathbf{v}_\theta(\mathbf{x}_t, t) - \mathbf{v}^*(\mathbf{x}_t, t \mid \mathbf{x}_1)\right\|^2\right],
    \label{eq:flow_matching}
\end{align}
where $t \sim \mathcal{U}(0, 1)$, $\mathbf{x}_1 \sim p_{\text{data}}$, and $\mathbf{x}_t \sim p_t(\mathbf{x} \mid \mathbf{x}_1)$. Crucially, the conditional vector field $\mathbf{v}^*(\mathbf{x}_t, t \mid \mathbf{x}_1)$ is known in closed form (Equation~\ref{eq:cond_vector_field}), making the objective tractable.

Training proceeds by:
\begin{enumerate}
    \item Sampling a data point $\mathbf{x}_1 \sim p_{\text{data}}$ and noise $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$,
    \item Sampling a time step $t \sim \mathcal{U}(0, 1)$,
    \item Constructing $\mathbf{x}_t = \alpha(t)\,\mathbf{x}_1 + \sigma(t)\,\boldsymbol{\epsilon}$,
    \item Regressing the neural network output toward the conditional velocity $\mathbf{v}^*(\mathbf{x}_t, t \mid \mathbf{x}_1)$.
\end{enumerate}
This reduces generative modeling to \emph{supervised regression on vector fields}. For the special case of \emph{rectified flow}~\cite{liu2023flow}, where $\alpha(t) = t$ and $\sigma(t) = 1 - t$, the conditional vector field simplifies to $\mathbf{v}^* = \mathbf{x}_1 - \boldsymbol{\epsilon}$, yielding straight-line interpolation paths between noise and data.

\subsection{Stochastic Differential Equations}
\label{subsec:sde}

Diffusion models generalize the deterministic flow framework by introducing stochasticity. Instead of an ODE, the forward process is described by a Stochastic Differential Equation (SDE)~\cite{song2021scorebased}:
\begin{align}
    d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)\, dt + g(t)\, d\mathbf{w}_t,
    \label{eq:forward_sde}
\end{align}
where $\mathbf{f}(\mathbf{x}, t)$ is the \emph{drift} coefficient, $g(t)$ is the \emph{diffusion} coefficient controlling the noise magnitude, and $d\mathbf{w}_t$ denotes standard Brownian motion. The drift determines the deterministic trajectory of the process, while the diffusion term injects random perturbations at each infinitesimal time step.

The corresponding evolution of the probability density is governed by the \emph{Fokker--Planck equation}:
\begin{align}
    \frac{\partial p_t}{\partial t} = -\nabla \cdot (\mathbf{f}\, p_t) + \frac{1}{2}\,g(t)^2\,\Delta p_t,
    \label{eq:fokker_planck}
\end{align}
where $\Delta = \nabla \cdot \nabla$ is the Laplacian operator. Compared to the continuity equation~\eqref{eq:continuity}, the additional term $\frac{1}{2}g(t)^2 \Delta p_t$ introduces stochastic spreading of probability mass. This diffusion term is what gives diffusion models their name.

\subsection{Reverse-Time SDE and Score Functions}
\label{subsec:reverse_sde}

A fundamental result by Anderson~\cite{anderson1982reverse} shows that the reverse-time dynamics of the forward SDE~\eqref{eq:forward_sde} is itself an SDE:
\begin{align}
    d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - g(t)^2\,\nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right] dt + g(t)\, d\bar{\mathbf{w}}_t,
    \label{eq:reverse_sde}
\end{align}
where $d\bar{\mathbf{w}}_t$ denotes reverse-time Brownian motion. The critical quantity
\begin{align}
    \mathbf{s}(\mathbf{x}, t) \coloneqq \nabla_{\mathbf{x}} \log p_t(\mathbf{x})
    \label{eq:score_function}
\end{align}
is called the \emph{score function}---the gradient of the log-probability density at time $t$. The score points in the direction of increasing probability and its magnitude indicates how steeply the density changes. Intuitively, the reverse SDE follows the drift of the forward process but adds a correction term proportional to the score that ``steers'' samples toward regions of higher data probability.

Diffusion models approximate the score using a neural network $\mathbf{s}_\theta(\mathbf{x}, t) \approx \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$, trained via \emph{denoising score matching}~\cite{song2021scorebased}:
\begin{align}
    \mathcal{L}_{\text{SM}} = \mathbb{E}_{t,\, \mathbf{x}_0,\, \mathbf{x}_t}\!\left[\left\|\mathbf{s}_\theta(\mathbf{x}_t, t) - \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{x}_0)\right\|^2\right].
    \label{eq:score_matching}
\end{align}
For Gaussian conditional paths, $\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{x}_0) = -\boldsymbol{\epsilon} / \sigma(t)$, and this objective becomes equivalent to the noise-prediction loss used in DDPMs (see Section~\ref{subsec:ddpm}).

\paragraph{Probability Flow ODE.}
Song et al.~\cite{song2021scorebased} further showed that any diffusion SDE has a corresponding deterministic ODE---the \emph{probability flow ODE}---that induces the same marginal densities $p_t(\mathbf{x})$:
\begin{align}
    \frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}\,g(t)^2\,\nabla_{\mathbf{x}} \log p_t(\mathbf{x}).
    \label{eq:prob_flow_ode}
\end{align}
This ODE enables deterministic sampling with numerical ODE solvers (e.g., Euler, Heun, or adaptive-step methods), offering a trade-off between stochastic diversity and deterministic speed. It also enables exact likelihood computation via the instantaneous change-of-variables formula.

\subsection{Conditional Generation and Guidance}
\label{subsec:cfg}

In many applications, we wish to condition generation on an external variable $\mathbf{c}$ (e.g., text descriptions, bounding boxes, semantic maps, or trajectories). The conditional score decomposes via Bayes' rule:
\begin{align}
    \nabla_{\mathbf{x}} \log p_t(\mathbf{x} \mid \mathbf{c}) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) + \nabla_{\mathbf{x}} \log p(\mathbf{c} \mid \mathbf{x}).
    \label{eq:conditional_score}
\end{align}
\emph{Classifier guidance}~\cite{dhariwal2021diffusion} approximates the second term using an auxiliary classifier trained on noisy data. However, this requires training a separate classifier and may introduce artifacts.

\emph{Classifier-free guidance} (CFG)~\cite{ho2022classifierfree} avoids this by jointly training conditional and unconditional score estimates within a single model. During training, the conditioning $\mathbf{c}$ is randomly dropped with some probability, so the model learns both $\mathbf{s}_\theta(\mathbf{x}, t, \mathbf{c})$ and $\mathbf{s}_\theta(\mathbf{x}, t, \varnothing)$. During inference, the two estimates are combined:
\begin{align}
    \tilde{\mathbf{s}}_\theta(\mathbf{x}, t, \mathbf{c}) = (1 + w)\,\mathbf{s}_\theta(\mathbf{x}, t, \mathbf{c}) - w\,\mathbf{s}_\theta(\mathbf{x}, t, \varnothing),
    \label{eq:cfg}
\end{align}
where $w > 0$ is the \emph{guidance scale}. Higher values of $w$ increase adherence to the condition at the expense of sample diversity. CFG is widely used in modern diffusion models including Stable Diffusion and Stable Video Diffusion, and is employed in both stages of the pipeline presented in this thesis.

\subsection{Unified View}
\label{subsec:unified_view}

Both flow matching and diffusion models can be understood within a unified differential equation framework~\cite{lipman2023flow, song2021scorebased, albergo2023stochastic}:

\begin{itemize}
    \item \textbf{Flow models} learn deterministic vector fields $\mathbf{v}_\theta$ and generate samples by solving ODEs (Equation~\ref{eq:flow_ode}).
    \item \textbf{Diffusion models} learn score functions $\mathbf{s}_\theta$ governing reverse-time SDEs (Equation~\ref{eq:reverse_sde}) or their equivalent probability flow ODEs (Equation~\ref{eq:prob_flow_ode}).
    \item \textbf{Discrete diffusion models} (e.g., DDPM~\cite{ho2020denoising}) arise from time-discretization of continuous SDEs with specific choices of $\mathbf{f}$ and $g$.
\end{itemize}

The connection between flow matching and diffusion is made precise by noting that the score function and the marginal vector field are related via:
\begin{align}
    \mathbf{v}^*(\mathbf{x}_t, t) = \mathbf{f}(\mathbf{x}_t, t) - \frac{1}{2}\,g(t)^2\,\nabla_{\mathbf{x}} \log p_t(\mathbf{x}_t),
    \label{eq:flow_score_relation}
\end{align}
which is exactly the probability flow ODE~\eqref{eq:prob_flow_ode}. This perspective provides a coherent mathematical foundation for modern generative modeling and naturally extends to conditional and controllable generation settings, as exploited throughout this thesis.

\subsection{Denoising Diffusion Probabilistic Models}
\label{subsec:ddpm}

Denoising Diffusion Probabilistic Models (DDPMs)~\cite{ho2020denoising, sohldickstein2015deep} provide a concrete, discrete-time instantiation of the continuous framework presented above. The forward process is defined by a Markov chain with Gaussian transitions:
\begin{align}
    q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\!\left(\mathbf{x}_t;\, \sqrt{1 - \beta_t}\,\mathbf{x}_{t-1},\, \beta_t\,\mathbf{I}\right),
    \label{eq:forward_step}
\end{align}
where $\{\beta_t\}_{t=1}^T$ is a variance schedule. Defining $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$, the noisy sample at any step $t$ admits a closed-form expression:
\begin{align}
    q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}\!\left(\mathbf{x}_t;\, \sqrt{\bar{\alpha}_t}\,\mathbf{x}_0,\, (1 - \bar{\alpha}_t)\,\mathbf{I}\right),
    \label{eq:forward_closed}
\end{align}
which corresponds to the continuous Gaussian path~\eqref{eq:gaussian_interpolant} with $\alpha(t) = \sqrt{\bar{\alpha}_t}$ and $\sigma(t) = \sqrt{1 - \bar{\alpha}_t}$.

The reverse process is parameterized as:
\begin{align}
    p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}\!\left(\mathbf{x}_{t-1};\, \boldsymbol{\mu}_\theta(\mathbf{x}_t, t),\, \sigma_t^2\,\mathbf{I}\right).
    \label{eq:reverse}
\end{align}
Ho et al.~\cite{ho2020denoising} showed that parameterizing the model to predict the noise $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$ rather than the mean directly leads to a simplified training objective:
\begin{align}
    \mathcal{L}_{\text{simple}} = \mathbb{E}_{t,\, \mathbf{x}_0,\, \boldsymbol{\epsilon}}\!\left[\left\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\right\|^2\right],
    \label{eq:ddpm_loss}
\end{align}
which is equivalent to the score matching objective~\eqref{eq:score_matching} up to a time-dependent weighting factor, since $\boldsymbol{\epsilon} = -\sigma(t)\,\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t \mid \mathbf{x}_0)$. This connection confirms that DDPM training is a discretized form of score function learning.

Subsequent improvements by Nichol and Dhariwal~\cite{nichol2021improved} introduced learned variance schedules and importance-weighted training, while Dhariwal and Nichol~\cite{dhariwal2021diffusion} demonstrated that diffusion models surpass GANs~\cite{goodfellow2014generative} in image generation quality when combined with classifier guidance.

% -----------------------------------------------------------------
\section{Latent Diffusion Models}
\label{sec:ldm}

\subsection{Motivation and Architecture}
\label{subsec:ldm_motivation}

Applying diffusion directly in pixel space is computationally expensive, particularly for high-resolution images and video sequences. The cost scales with spatial resolution, as the denoising network must operate on full-resolution tensors at every diffusion step.

Rombach et al.~\cite{rombach2022highresolution} introduced \emph{Latent Diffusion Models} (LDMs), which perform diffusion in a learned latent space rather than in pixel space. The key idea is to separate:
\begin{itemize}
    \item Perceptual compression, and
    \item Generative modeling.
\end{itemize}

\paragraph{Perceptual Compression via VAE.}
A variational autoencoder (VAE) is first trained to encode images into a lower-dimensional latent representation. Let $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$ denote an image. The encoder $\mathcal{E}$ maps it to a latent tensor:
\begin{align}
    \mathbf{z} = \mathcal{E}(\mathbf{x}) \in \mathbb{R}^{h \times w \times c},
    \label{eq:vae_encode}
\end{align}
where $h = H/f$, $w = W/f$, and $f$ is a spatial downsampling factor (typically $f=8$). The decoder $\mathcal{D}$ reconstructs the image:
\begin{align}
    \hat{\mathbf{x}} = \mathcal{D}(\mathbf{z}),
    \label{eq:vae_decode}
\end{align}
such that $\mathcal{D}(\mathcal{E}(\mathbf{x})) \approx \mathbf{x}$. The VAE is trained with a combination of reconstruction and perceptual losses to ensure that the latent representation preserves semantically relevant structure.

By operating in latent space, the dimensionality is reduced by a factor of $f^2$, significantly lowering computational cost.

\paragraph{Diffusion in Latent Space.}
After training the VAE, diffusion is performed on latent variables:
\begin{align}
    \mathbf{z}_0 = \mathcal{E}(\mathbf{x}).
    \label{eq:latent_init}
\end{align}
The forward and reverse diffusion processes are then applied to $\mathbf{z}$ rather than $\mathbf{x}$:
\begin{align}
    q(\mathbf{z}_t \mid \mathbf{z}_0), \quad p_\theta(\mathbf{z}_{t-1} \mid \mathbf{z}_t).
    \label{eq:latent_diffusion}
\end{align}
The training objective remains identical to pixel-space diffusion (e.g., noise prediction or score matching as in Equations~\eqref{eq:ddpm_loss} and~\eqref{eq:score_matching}), but the model now learns to denoise latent representations instead of raw pixels.

At inference time:
\begin{enumerate}
    \item Sample $\mathbf{z}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$,
    \item Iteratively denoise to obtain $\mathbf{z}_0$,
    \item Decode using $\mathcal{D}$ to produce the final image.
\end{enumerate}

\paragraph{Conditioning via Cross-Attention.}
The denoising network in LDMs is typically a UNet~\cite{ronneberger2015unet} augmented with cross-attention layers~\cite{vaswani2017attention} to incorporate conditioning signals such as text embeddings, class labels, or spatial maps. Given intermediate UNet features projected to queries $\mathbf{Q}$, and conditioning embeddings projected to keys $\mathbf{K}$ and values $\mathbf{V}$, cross-attention is computed as:
\begin{align}
    \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\right)\mathbf{V},
    \label{eq:cross_attention}
\end{align}
where $d$ is the dimensionality of the attention head. This mechanism allows the generative process to be guided by external information while maintaining spatial consistency in the latent representation.

\paragraph{Relevance to Video Diffusion.}
Latent diffusion significantly reduces computational cost and memory usage, enabling extension to high-resolution images and spatio-temporal video models. Stable Diffusion~\cite{rombach2022highresolution} and its video extensions build upon this architecture, and the Ctrl-V framework adopts the same latent-space diffusion backbone. Operating in latent space is therefore essential for scalable controllable video generation.

\subsection{EDM Noise Scheduling}
\label{subsec:edm}

In the continuous-time formulation presented above, diffusion models are defined through a stochastic differential equation (SDE) that gradually perturbs data toward Gaussian noise. In practice, early diffusion models such as DDPM discretized this process using a predefined variance schedule $\{\beta_t\}$ indexed by time $t$.

While effective, this time-based parameterization introduces several limitations:
\begin{itemize}
    \item The noise level is indirectly controlled by discrete time indices,
    \item The dynamic range of signal-to-noise ratios varies significantly across timesteps,
    \item Training stability depends heavily on the choice of variance schedule,
    \item Sampling procedures are tightly coupled to the discretization design.
\end{itemize}

To address these issues, Karras et al.~\cite{karras2022elucidating} proposed the EDM (Elucidating the Design Space of Diffusion Models) framework, which reformulates diffusion directly in terms of a continuous noise scale $\sigma$.

\paragraph{Continuous Noise Parameterization.}
Instead of indexing the forward process by time $t$, EDM defines corruption using an explicit noise magnitude:
\begin{align}
    \mathbf{x}_\sigma = \mathbf{x}_0 + \sigma\,\boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
    \label{eq:edm_noise}
\end{align}
where $\mathbf{x}_0$ is a clean sample and $\sigma > 0$ directly controls the signal-to-noise ratio. This formulation can be interpreted as a variance-exploding SDE discretized by noise scale rather than time. Small $\sigma$ corresponds to near-clean samples, while large $\sigma$ approximates pure noise.

By using $\sigma$ as the primary parameter, the diffusion process becomes independent of a specific time discretization. This provides greater flexibility in both training and sampling.

\paragraph{Preconditioned Denoiser.}
Another key insight of EDM is that a single neural network must operate across a wide range of noise levels. Without appropriate scaling, the network faces unstable gradients and poorly conditioned inputs. EDM therefore introduces a preconditioned denoiser:
\begin{align}
    D_\theta(\mathbf{x}_\sigma; \sigma) = c_{\text{skip}}(\sigma)\,\mathbf{x}_\sigma + c_{\text{out}}(\sigma)\,F_\theta\!\left(c_{\text{in}}(\sigma)\,\mathbf{x}_\sigma;\, c_{\text{noise}}(\sigma)\right),
    \label{eq:edm_denoiser}
\end{align}
where the scaling functions normalize the input and output across different noise magnitudes. This design ensures:
\begin{itemize}
    \item Stable gradients across noise scales,
    \item Improved numerical conditioning,
    \item Decoupling between noise scheduling and network architecture.
\end{itemize}

\paragraph{Relation to the Continuous Diffusion Framework.}
From the perspective of Section~\ref{sec:diffusion_models}, EDM can be viewed as:
\begin{itemize}
    \item Choosing a specific probability path $p_\sigma(\mathbf{x})$,
    \item Learning the reverse-time dynamics using a preconditioned score or denoiser,
    \item Parameterizing the process in terms of noise level instead of abstract time.
\end{itemize}

Modern large-scale diffusion systems, including Stable Diffusion and Stable Video Diffusion~\cite{blattmann2023stable}, adopt this formulation. The Ctrl-V framework~\cite{luo2025ctrlv} used in this work inherits the EDM noise parameterization, making it directly compatible with continuous-time diffusion theory.

% -----------------------------------------------------------------
\section{Variational Autoencoders}
\label{sec:vae}

\subsection{Theoretical Foundation}
\label{subsec:vae_theory}

The Variational Autoencoder (VAE), introduced independently by Kingma and Welling~\cite{kingma2014autoencoding} and Rezende et al.~\cite{rezende2014stochastic}, is a generative model that learns a latent representation $\mathbf{z}$ of data $\mathbf{x}$ by maximizing a variational lower bound on the log-likelihood:
\begin{align}
    \log p(\mathbf{x}) \geq \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\!\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right] - D_{\text{KL}}\!\left(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})\right) = \mathcal{L}_{\text{ELBO}},
    \label{eq:elbo}
\end{align}
where $q_\phi(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\boldsymbol{\mu}_\phi(\mathbf{x}), \boldsymbol{\sigma}_\phi^2(\mathbf{x})\,\mathbf{I})$ is the approximate posterior (encoder), $p_\theta(\mathbf{x}|\mathbf{z})$ is the likelihood (decoder), and $p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I})$ is the prior. Training maximizes $\mathcal{L}_{\text{ELBO}}$ with respect to both encoder parameters $\phi$ and decoder parameters $\theta$.

\paragraph{Reparameterization Trick.}
To enable backpropagation through the stochastic sampling of $\mathbf{z}$, Kingma and Welling introduced the reparameterization trick:
\begin{align}
    \mathbf{z} = \boldsymbol{\mu}_\phi(\mathbf{x}) + \boldsymbol{\sigma}_\phi(\mathbf{x}) \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
    \label{eq:reparam}
\end{align}
which expresses $\mathbf{z}$ as a deterministic function of $\phi$ and the random variable $\boldsymbol{\epsilon}$.

\paragraph{KL Divergence.}
For a Gaussian approximate posterior and a standard Gaussian prior, the KL divergence admits a closed-form expression:
\begin{align}
    D_{\text{KL}}\!\left(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})\right) = -\frac{1}{2}\sum_{j=1}^{J}\left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right),
    \label{eq:kl_closed}
\end{align}
where $J$ is the dimensionality of the latent space. This term acts as a regularizer, encouraging the approximate posterior to stay close to the prior and ensuring a smooth, well-structured latent space.

\subsection{VAE in Latent Diffusion Models}
\label{subsec:vae_in_ldm}

In the LDM framework~\cite{rombach2022highresolution}, the VAE serves as a perceptual compression model that maps high-dimensional images into a lower-dimensional latent space. The VAE used in Stable Diffusion employs a KL-regularized autoencoder trained with a combination of:
\begin{itemize}
    \item Pixel-level reconstruction loss ($L_1$ or $L_2$),
    \item Perceptual loss (LPIPS)~\cite{zhang2018unreasonable},
    \item Adversarial loss (patch-based discriminator),
    \item KL divergence regularization.
\end{itemize}

The encoder produces latent codes $\mathbf{z} \in \mathbb{R}^{h \times w \times 4}$ with a spatial downsampling factor of 8, and the decoder reconstructs images from these latents. Crucially, the VAE is trained once and then frozen during diffusion model training, which means all diffusion operations occur in the fixed latent space.

\subsection{3D VAE for Video}
\label{subsec:3d_vae}

Stable Video Diffusion (SVD)~\cite{blattmann2023stable} extends the image VAE to handle video sequences. The architecture uses a \textbf{2D encoder} that processes each frame independently, producing per-frame latents $\mathbf{z}^{(i)} = \mathcal{E}(\mathbf{f}^{(i)})$, and a \textbf{3D temporal decoder} that jointly decodes all frame latents while maintaining temporal coherence through 3D convolutions:
\begin{align}
    \mathcal{E}&: \mathbb{R}^{3 \times H \times W} \rightarrow \mathbb{R}^{4 \times \frac{H}{8} \times \frac{W}{8}}, \quad \text{(per-frame, 2D)} \label{eq:svd_enc}\\
    \mathcal{D}&: \mathbb{R}^{T \times 4 \times \frac{H}{8} \times \frac{W}{8}} \rightarrow \mathbb{R}^{T \times 3 \times H \times W}. \quad \text{(temporal, 3D)} \label{eq:svd_dec}
\end{align}

This asymmetric design---2D encoder, 3D decoder---is a key architectural choice that enables efficient encoding while ensuring temporal smoothness in decoded video sequences. The encoder consists of residual blocks with downsampling, a mid-block with self-attention, and final convolutional layers that map from 128 internal channels to $2 \times 4 = 8$ output channels (mean and log-variance for 4 latent channels). The decoder mirrors this structure with upsampling blocks but additionally includes temporal convolutions for inter-frame coherence.

\subsection{Semantic VAE}
\label{subsec:semantic_vae}

A central challenge addressed in this thesis is encoding discrete semantic segmentation maps through a VAE designed for continuous RGB images. Semantic maps consist of integer class labels (e.g., 0--18 for KITTI-360), which fundamentally differ from the smooth, continuous pixel intensities that the pretrained VAE expects.

\paragraph{The RGB Bottleneck Problem.}
A naive approach---converting semantic labels to an RGB color palette and passing through the standard VAE---suffers from an information bottleneck: 19 semantic classes must be compressed into 3 RGB channels. The VAE's learned smooth latent manifold further degrades sharp class boundaries, as the model was trained to reconstruct natural images with smooth gradients rather than discrete label maps. Experiments in Section~\ref{sec:vae_results} show that this approach achieves only 54.3\% mIoU despite 97.7\% pixel accuracy, revealing that boundaries account for a small fraction of pixels but a large fraction of errors.

\paragraph{Semantic-Native Architecture.}
To overcome this limitation, we propose a Semantic-Native VAE that bypasses the RGB input/output layers of the pretrained VAE. Instead of feeding 3-channel RGB inputs, the model introduces:
\begin{itemize}
    \item A \textbf{Semantic Stem} (trainable): Maps one-hot encoded semantic labels ($C=19$ channels) to the 128-channel feature space expected by the VAE encoder core.
    \item A \textbf{Semantic Head} (trainable): Maps the 128-channel decoder output back to $C=19$ class logits, using 3D convolutions for temporal consistency.
\end{itemize}

The frozen VAE encoder and decoder cores---comprising all layers between the input convolution and output convolution---are retained unchanged. This allows the model to leverage the powerful spatial representations learned during pretraining while adapting the input and output interfaces to the semantic domain.

% -----------------------------------------------------------------
\section{Video Diffusion Models}
\label{sec:video_diffusion}

\subsection{From Image to Video Diffusion}
\label{subsec:image_to_video}

Extending diffusion models from images to videos introduces the challenge of modeling temporal dynamics in addition to spatial content. Several strategies have been proposed:

\paragraph{Temporal Layers.}
Blattmann et al.~\cite{blattmann2023align} proposed inserting temporal attention and temporal convolution layers into a pretrained 2D UNet, then finetuning only these new layers on video data. This ``inflate-and-finetune'' approach preserves the spatial knowledge of the image model while learning temporal relationships.

\paragraph{Joint Space-Time Modeling.}
Ho et al.~\cite{ho2022video} proposed Video Diffusion Models that perform diffusion jointly over all frames using 3D UNet architectures, treating the video as a 4D tensor $\mathbf{x} \in \mathbb{R}^{T \times C \times H \times W}$.

\paragraph{Cascaded Models.}
Make-A-Video~\cite{singer2023makeavideo} decomposes the problem into separate spatial and temporal components, first generating a key frame and then interpolating or extrapolating to produce the full video sequence.

\subsection{Stable Video Diffusion}
\label{subsec:svd}

Stable Video Diffusion (SVD)~\cite{blattmann2023stable} is a latent video diffusion model that generates video sequences from a single conditioning image. Key design choices include:

\begin{itemize}
    \item \textbf{Architecture:} A UNet denoiser with interleaved spatial and temporal attention layers, operating on latent representations of size $T \times 4 \times H/8 \times W/8$.
    \item \textbf{Conditioning:} The first frame $\mathbf{f}^{(0)}$ is encoded via both the VAE encoder (providing spatial conditioning through concatenation with the noisy latents) and a CLIP image encoder~\cite{radford2021learning} (providing semantic conditioning through cross-attention).
    \item \textbf{Noise Schedule:} EDM-based scheduling~\cite{karras2022elucidating} with continuous noise levels.
    \item \textbf{Training Data Curation:} A systematic pipeline for selecting high-quality video clips with sufficient motion, appropriate aesthetics, and minimal artifacts.
\end{itemize}

SVD operates in image-to-video (I2V) mode: given a single input image, it generates a temporally coherent video continuation. The model variant SVD-XT generates 25 frames at resolutions up to $576 \times 1024$. This pretrained model serves as the backbone for the Ctrl-V framework and, by extension, for the work presented in this thesis.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/svd_architecture.png}
    \caption{Architecture overview of Stable Video Diffusion. The UNet denoiser operates in latent space and incorporates both spatial and temporal attention layers. The initial frame provides conditioning through both VAE-encoded latents and CLIP embeddings.}
    \label{fig:svd_architecture}
\end{figure}

% -----------------------------------------------------------------
\section{Controllable Video Generation}
\label{sec:controllable}

While diffusion models achieve impressive generative quality, they are inherently stochastic and primarily guided by weak conditioning signals such as text prompts. For applications such as autonomous driving video synthesis, robotics, or simulation, this is insufficient. In these domains, one requires \emph{explicit spatial and structural control}---enforcing object locations, trajectories, or scene layouts with precision.

Controllable diffusion models address this limitation by augmenting pretrained diffusion backbones with mechanisms that inject structured conditioning signals while preserving generative quality. This section reviews the key architectures that enable such control.

\subsection{ControlNet}
\label{subsec:controlnet}

\paragraph{Motivation.}
Pretrained diffusion models such as Stable Diffusion learn powerful image priors from large-scale datasets. However, fine-tuning the entire UNet to incorporate new conditioning signals often leads to:
\begin{itemize}
    \item Loss of pretrained generative quality,
    \item Overfitting on small control datasets,
    \item Catastrophic forgetting of the learned image prior.
\end{itemize}
ControlNet~\cite{zhang2023adding} was proposed to solve this problem by enabling spatial conditioning \emph{without modifying the pretrained backbone weights}.

\paragraph{Core Architecture.}
ControlNet creates a \emph{trainable copy} of the encoder blocks of a pretrained UNet while keeping the original UNet \emph{frozen}. Let:
\begin{itemize}
    \item $\mathcal{F}(\cdot;\, \Theta)$ be the pretrained (frozen) UNet block,
    \item $\mathcal{F}(\cdot;\, \Theta_c)$ be its trainable copy,
    \item $\mathbf{c}$ be the conditioning signal (e.g., edge map, depth map, segmentation mask),
    \item $\mathcal{Z}(\cdot;\, \Theta_z)$ be zero-initialized convolution layers.
\end{itemize}
The modified block output is:
\begin{align}
    \mathbf{y}_c = \mathcal{F}(\mathbf{x};\, \Theta) + \mathcal{Z}\!\left(\mathcal{F}\!\left(\mathbf{x} + \mathcal{Z}(\mathbf{c};\, \Theta_{z1});\, \Theta_c\right);\, \Theta_{z0}\right).
    \label{eq:controlnet}
\end{align}

\paragraph{Step-by-Step Interpretation.}
Equation~\eqref{eq:controlnet} can be understood through four sequential operations:

\begin{enumerate}
    \item \textbf{Baseline generation} --- $\mathcal{F}(\mathbf{x};\, \Theta)$: This is the original pretrained diffusion block. At initialization, the model behaves exactly like the original diffusion model, producing the same output as if no conditioning were present.

    \item \textbf{Conditioning injection} --- $\mathbf{x} + \mathcal{Z}(\mathbf{c};\, \Theta_{z1})$: The conditioning signal $\mathbf{c}$ is passed through a zero-initialized convolution layer. Because all weights are initialized to zero:
    \begin{align}
        \mathcal{Z}(\mathbf{c};\, \Theta_{z1}) = \mathbf{0} \quad \text{at initialization},
        \label{eq:zero_init}
    \end{align}
    and thus $\mathbf{x} + \mathbf{0} = \mathbf{x}$. The conditioning signal does not disturb the pretrained model at the start of training.

    \item \textbf{Trainable copy processing} --- $\mathcal{F}(\mathbf{x} + \mathcal{Z}(\mathbf{c});\, \Theta_c)$: This branch learns to transform the conditioning features into spatially meaningful information. Since $\Theta_c$ is initialized from $\Theta$, the trainable copy starts with the same representational capacity as the pretrained model.

    \item \textbf{Zero-initialized residual injection} --- $\mathcal{Z}(\cdot;\, \Theta_{z0})$: A second zero-initialized convolution ensures that the output of the trainable copy contributes nothing at initialization:
    \begin{align}
        \mathbf{y}_c = \mathcal{F}(\mathbf{x};\, \Theta) + \mathbf{0} = \mathcal{F}(\mathbf{x};\, \Theta) \quad \text{at initialization}.
        \label{eq:controlnet_init}
    \end{align}
\end{enumerate}

As training progresses, the parameters $\Theta_c$ learn meaningful control features, while $\Theta_{z0}$ and $\Theta_{z1}$ gradually allow these features to influence generation. This ensures that training \emph{starts from the pretrained solution} and the control signal is incorporated without destroying the learned prior.

\paragraph{Importance of Zero-Convolution.}
Without zero-initialization, the model output would change immediately upon adding the ControlNet branch, degrading pretrained generative quality from the first training step. Zero-convolutions guarantee that:
\begin{itemize}
    \item The pretrained model is perfectly preserved at initialization,
    \item The conditioning influence grows smoothly during training,
    \item Training is stable and data-efficient, even with small control datasets.
\end{itemize}
This design made ControlNet remarkably robust in practice and contributed to its widespread adoption.

\paragraph{Supported Control Modalities.}
ControlNet supports a wide range of spatial conditioning signals, including Canny edges, depth maps, human pose skeletons, segmentation masks, and scribbles. In practice, it transformed diffusion models from purely artistic generators into \emph{controllable image synthesis systems}, where the user can specify precise spatial structure while the diffusion model fills in realistic appearance and detail.

\paragraph{Extension to Video.}
Originally designed for image diffusion, ControlNet has been adapted to video diffusion models~\cite{hu2023videocontrolnet, chen2023controlavideo}. In the video setting:
\begin{itemize}
    \item Conditioning is applied independently per frame,
    \item The resulting control features are injected into a spatio-temporal UNet,
    \item Temporal attention layers maintain motion consistency across frames.
\end{itemize}
This extension enables spatial control that is coherent across time, which is essential for applications such as driving video generation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/controlnet_architecture.png}
    \caption{ControlNet architecture. A trainable copy of the UNet encoder receives the conditioning input and injects its features into the frozen UNet decoder via zero-initialized convolution layers. At initialization, the zero-convolutions ensure the model output is identical to the pretrained model.}
    \label{fig:controlnet_architecture}
\end{figure}

\subsection{Bounding-Box Control: Ctrl-V}
\label{subsec:ctrlv}

\paragraph{Motivation.}
In autonomous driving simulation, control via generic edges or depth maps is insufficient. Instead, the application demands:
\begin{itemize}
    \item Explicit object identities and categories,
    \item Bounding box trajectories specifying object motion,
    \item Temporal consistency across the generated video,
    \item Physical plausibility of object dynamics.
\end{itemize}
Ctrl-V~\cite{luo2025ctrlv} addresses these requirements by conditioning Stable Video Diffusion (SVD) on bounding box trajectories through a two-stage framework.

\paragraph{Stage~1 --- Bounding Box Generator.}
The first stage predicts bounding box trajectories using a diffusion model built on the SVD backbone. Given:
\begin{itemize}
    \item An initial RGB frame $\mathbf{f}^{(0)}$,
    \item An initial bounding box frame $\mathbf{b}^{(0)}$,
    \item Optionally, a final bounding box frame $\mathbf{b}^{(N-1)}$,
\end{itemize}
the model generates the full bounding box video sequence:
\begin{align}
    \mathbf{b} = \left[\mathbf{b}^{(0)},\, \mathbf{b}^{(1)},\, \ldots,\, \mathbf{b}^{(N-1)}\right]
    \label{eq:bbox_sequence}
\end{align}
through the diffusion process in latent space. This stage converts sparse control inputs into a complete temporal trajectory, providing dense frame-by-frame conditioning for the subsequent video generation stage.

\paragraph{Stage~2 --- Box2Video Generation.}
The predicted bounding box sequence is encoded via the VAE and injected into SVD through a ControlNet module. The noise prediction combines the frozen SVD backbone with the trainable ControlNet:
\begin{align}
    \boldsymbol{\epsilon}_\theta(\hat{\mathbf{z}}_t, t, \mathbf{z}^{(0)}, \mathbf{c}^{(0)}, \mathbf{b}) = \mathbb{U}_\theta\!\left(\hat{\mathbf{z}}_t, t, \mathbf{z}^{(0)}_{\text{pad}}, \mathbf{c}^{(0)}\right) + \text{ControlNet}_\xi(\hat{\mathbf{z}}_t, \mathbf{b}),
    \label{eq:ctrlv_box2video}
\end{align}
where:
\begin{itemize}
    \item $\mathbb{U}_\theta$ is the frozen SVD UNet backbone,
    \item $\hat{\mathbf{z}}_t$ is the noisy video latent at diffusion step $t$,
    \item $\mathbf{z}^{(0)}_{\text{pad}}$ is the initial frame latent padded along the temporal dimension,
    \item $\mathbf{c}^{(0)}$ is the CLIP embedding of the initial frame,
    \item $\mathbf{b}$ is the VAE-encoded bounding box sequence,
    \item $\xi$ denotes the trainable ControlNet parameters.
\end{itemize}

The key design property is that $\theta$ \emph{remains frozen} while only $\xi$ \emph{is trained}. This preserves the pretrained video generation prior intact; only the control branch adapts to the bounding box conditioning. The bounding box trajectories guide object motion while the frozen SVD backbone ensures high visual quality and temporal coherence.

\paragraph{Advantages of Bounding Box Control.}
Compared to pixel-level conditioning (e.g., dense segmentation maps), bounding boxes provide:
\begin{itemize}
    \item Explicit object-level control with interpretable structure,
    \item Compact representation that scales to long sequences,
    \item Direct compatibility with object detection datasets commonly used in autonomous driving.
\end{itemize}
This makes Ctrl-V particularly suitable for driving simulation, where object trajectories are a natural and available form of supervision.

\subsection{Other Control Modalities}
\label{subsec:other_controls}

Beyond bounding boxes, various spatial and motion control strategies have been explored for video generation:
\begin{itemize}
    \item \textbf{Depth and Edge Maps:} VideoControlNet~\cite{hu2023videocontrolnet} uses per-frame depth, Canny edges, and optical flow as frame-level conditions for video diffusion.
    \item \textbf{Motion Modules:} AnimateDiff~\cite{guo2024animatediff} introduces learnable motion layers that can be conditioned on motion trajectories, enabling temporal dynamics control.
    \item \textbf{Text and Layout:} Boximator~\cite{wang2024boximator} combines textual prompts with hard and soft bounding box constraints for fine-grained motion control.
    \item \textbf{Semantic Layouts:} SPADE-based approaches~\cite{park2019semantic} condition image synthesis on semantic segmentation maps, but their extension to video generation within diffusion frameworks remains underexplored---a gap that this thesis aims to fill.
\end{itemize}

\subsection{Driving-Specific Video Generation}
\label{subsec:driving_generation}

Several recent works target video generation for autonomous driving specifically:
\begin{itemize}
    \item \textbf{GAIA-1}~\cite{hu2023gaia}: A generative world model that produces driving videos from text, action, and video tokens.
    \item \textbf{DrivingDiffusion}~\cite{li2023drivingdiffusion}: Uses layout-guided multi-view generation with latent diffusion.
    \item \textbf{VISTA}~\cite{gao2024vista}: A generalizable driving world model with high fidelity and versatile controllability.
\end{itemize}

These works highlight the growing interest in using generative models for autonomous driving simulation. The approach in this thesis differs by using \emph{dense semantic segmentation maps} as the control signal, providing pixel-level scene specification rather than sparse bounding boxes or text descriptions.

\paragraph{Summary.}
ControlNet introduced a principled mechanism for injecting structured spatial control into pretrained diffusion models without degrading generative quality. Ctrl-V extends this idea to video generation with bounding box trajectories, enabling object-level control and temporal consistency for autonomous driving simulation. These architectures form the foundation for controllable video synthesis and directly motivate the semantic control mechanisms proposed in this thesis.
