% =================================================================
% CHAPTER 3: METHODOLOGY
% =================================================================
\chapter{Methodology}
\label{ch:methodology}

This chapter describes the dataset, evaluation metrics, the proposed model architectures, and the loss functions employed in this work.

% -----------------------------------------------------------------
\section{Dataset: KITTI-360}
\label{sec:dataset}

\subsection{Overview}
\label{subsec:dataset_overview}

The KITTI-360 dataset~\cite{liao2022kitti360} is a large-scale outdoor dataset for autonomous driving research, extending the original KITTI benchmark~\cite{geiger2012are} with richer annotations including dense semantic and instance segmentation, 3D bounding boxes, and accumulated point clouds. The dataset was captured in suburban areas of Karlsruhe, Germany, using a Volkswagen station wagon equipped with:
\begin{itemize}
    \item Two high-resolution color cameras (stereo),
    \item A Velodyne HDL-64E 3D laser scanner,
    \item An OXTS RT3003 GPS/IMU localization unit.
\end{itemize}

For this work, we use the front-facing camera (camera~00) images and corresponding semantic segmentation annotations. The dataset is split into 9 driving sequences covering a total of approximately 73,000 frames.

\subsection{Semantic Annotations}
\label{subsec:semantic_annotations}

The semantic annotations use 19 training classes consistent with the Cityscapes label definition~\cite{cordts2016cityscapes}. Raw KITTI-360 label IDs are remapped to continuous training IDs (0--18) using the official \texttt{kitti360scripts} library. Table~\ref{tab:kitti360_classes} lists the semantic classes used.

\begin{table}[t]
    \centering
    \caption{KITTI-360 semantic classes with training IDs used in this work}
    \label{tab:kitti360_classes}
    \begin{tabular}{clcl}
        \toprule
        \textbf{Train ID} & \textbf{Class} & \textbf{Train ID} & \textbf{Class} \\
        \midrule
        0  & Road         & 10 & Sky \\
        1  & Sidewalk     & 11 & Person \\
        2  & Building     & 12 & Rider \\
        3  & Wall         & 13 & Car \\
        4  & Fence        & 14 & Truck \\
        5  & Pole         & 15 & Bus \\
        6  & Traffic Light & 16 & Train \\
        7  & Traffic Sign & 17 & Motorcycle \\
        8  & Vegetation   & 18 & Bicycle \\
        9  & Terrain      &    & \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Data Preparation}
\label{subsec:data_preparation}

The official train/validation split files (\texttt{2013\_05\_28\_drive\_train\_frames.txt} and \texttt{2013\_05\_28\_drive\_val\_frames.txt}) define paired RGB and semantic paths. Each pair specifies an RGB image from \texttt{data\_2d\_raw/} and its corresponding semantic label map from \texttt{data\_2d\_semantics/train/}.

Key preprocessing steps:
\begin{itemize}
    \item \textbf{Resolution:} Images are resized from the original $376 \times 1408$ to $192 \times 704$ using bilinear interpolation for RGB and nearest-neighbor interpolation for semantic maps (to preserve discrete labels).
    \item \textbf{Clip Formation:} Consecutive frames are grouped into clips of $T=25$ frames for diffusion training and $T=4$ frames for Semantic VAE training.
    \item \textbf{Normalization:} RGB images are normalized to $[-1, 1]$; semantic labels remain as integer IDs.
\end{itemize}

The training set contains approximately 49,000 frame pairs. For Semantic VAE training, a subset of 500 clips (2,000 frames at $T=4$) is used due to the small number of trainable parameters, with 200 clips reserved for validation.

% -----------------------------------------------------------------
\section{Evaluation Metrics}
\label{sec:metrics}

\subsection{Fr\'echet Inception Distance (FID)}
\label{subsec:fid}

The Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} measures the similarity between the distributions of generated and real images in the feature space of an Inception-v3 network~\cite{szegedy2016rethinking}. Given multivariate Gaussian fits $\mathcal{N}(\boldsymbol{\mu}_r, \boldsymbol{\Sigma}_r)$ and $\mathcal{N}(\boldsymbol{\mu}_g, \boldsymbol{\Sigma}_g)$ to the real and generated feature distributions respectively:
\begin{align}
    \text{FID} = \|\boldsymbol{\mu}_r - \boldsymbol{\mu}_g\|^2 + \text{Tr}\!\left(\boldsymbol{\Sigma}_r + \boldsymbol{\Sigma}_g - 2\left(\boldsymbol{\Sigma}_r \boldsymbol{\Sigma}_g\right)^{1/2}\right).
    \label{eq:fid}
\end{align}
Lower FID scores indicate higher similarity between generated and real image distributions. FID is computed on individual frames extracted from generated and ground-truth videos.

\subsection{Fr\'echet Video Distance (FVD)}
\label{subsec:fvd}

The Fr\'echet Video Distance (FVD)~\cite{unterthiner2019fvd} extends FID to the video domain by using features from an Inflated 3D ConvNet (I3D)~\cite{carreira2017quo} pretrained on Kinetics-400. The I3D network captures both spatial appearance and temporal dynamics:
\begin{align}
    \text{FVD} = \|\boldsymbol{\mu}_r^{\text{I3D}} - \boldsymbol{\mu}_g^{\text{I3D}}\|^2 + \text{Tr}\!\left(\boldsymbol{\Sigma}_r^{\text{I3D}} + \boldsymbol{\Sigma}_g^{\text{I3D}} - 2\left(\boldsymbol{\Sigma}_r^{\text{I3D}} \boldsymbol{\Sigma}_g^{\text{I3D}}\right)^{1/2}\right).
    \label{eq:fvd}
\end{align}
FVD evaluates both the visual quality and temporal coherence of generated videos. Lower values are better.

\subsection{Mean Intersection-over-Union (mIoU)}
\label{subsec:miou}

For evaluating the Semantic VAE, the Mean Intersection-over-Union is the primary metric. For a given class $c$:
\begin{align}
    \text{IoU}_c = \frac{|\mathcal{P}_c \cap \mathcal{T}_c|}{|\mathcal{P}_c \cup \mathcal{T}_c|} = \frac{\text{TP}_c}{\text{TP}_c + \text{FP}_c + \text{FN}_c},
    \label{eq:iou}
\end{align}
where $\mathcal{P}_c$ and $\mathcal{T}_c$ are the sets of pixels predicted and labeled as class $c$, respectively. The mean IoU averages over all valid classes:
\begin{align}
    \text{mIoU} = \frac{1}{|\mathcal{C}_{\text{valid}}|} \sum_{c \in \mathcal{C}_{\text{valid}}} \text{IoU}_c.
    \label{eq:miou}
\end{align}
Unlike pixel accuracy, mIoU treats all classes equally regardless of their spatial prevalence, making it sensitive to performance on rare classes such as poles and traffic signs.

\subsection{Additional Video Quality Metrics}
\label{subsec:additional_metrics}

In addition to FID and FVD, we report several complementary metrics:

\begin{itemize}
    \item \textbf{LPIPS} (Learned Perceptual Image Patch Similarity)~\cite{zhang2018unreasonable}: Measures perceptual distance between image pairs using deep features. Lower is better.
    \item \textbf{SSIM} (Structural Similarity Index): Evaluates structural similarity between generated and ground-truth frames, considering luminance, contrast, and structure. Higher is better, with a maximum of 1.0.
    \item \textbf{PSNR} (Peak Signal-to-Noise Ratio): Measures pixel-level reconstruction quality in decibels. Higher values indicate less distortion.
\end{itemize}

% -----------------------------------------------------------------
\section{Model Architecture}
\label{sec:architecture}

\subsection{Overview of the Two-Stage Pipeline}
\label{subsec:pipeline_overview}

The proposed pipeline, illustrated in Figure~\ref{fig:pipeline_overview}, consists of three components:

\begin{enumerate}
    \item \textbf{Semantic VAE} (pretrained, frozen during diffusion training): Encodes semantic label maps into and decodes them from the latent space shared with the RGB VAE.
    \item \textbf{Stage~1 --- Semantic Prediction Model:} A diffusion model that predicts future semantic video latents given an initial RGB frame.
    \item \textbf{Stage~2 --- Semantic-to-Video Model:} A ControlNet-conditioned diffusion model that generates RGB video latents conditioned on semantic video latents.
\end{enumerate}

\begin{figure}[t]
    \centering
    % TODO: Replace with actual pipeline diagram
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{3cm}\textbf{[Pipeline Overview Diagram]}\\\textit{Two-stage pipeline: Semantic VAE + Stage~1 Semantic Prediction + Stage~2 Semantic-to-Video}\vspace{3cm}}}
    \caption{Overview of the proposed two-stage pipeline for semantically controlled video generation. The Semantic VAE bridges discrete semantic maps and continuous latent space. Stage~1 predicts future semantic sequences. Stage~2 generates photorealistic RGB video conditioned on the predicted semantics.}
    \label{fig:pipeline_overview}
\end{figure}

\subsection{Semantic-Native VAE}
\label{subsec:semantic_native_vae}

The pretrained Stable Video Diffusion (SVD) VAE was trained on natural RGB images and therefore assumes an input tensor $\mathbf{x}_{\text{rgb}} \in \mathbb{R}^{B \times T \times 3 \times H \times W}$. Internally, the VAE begins with a learned projection that converts 3 RGB channels into the feature width expected by the encoder backbone (128 channels), and ends with a symmetric projection from 128-channel decoder features back to 3 RGB channels. However, semantic label maps are \emph{discrete} variables
\begin{align}
    \mathbf{y} \in \{0, \ldots, C-1\}^{B \times T \times H \times W}, \quad C = 19,
    \label{eq:semantic_input}
\end{align}
and do not admit a meaningful representation in 3 channels without information loss. To bridge this mismatch, we introduce a \emph{Semantic-Native VAE} that replaces only the RGB-specific input and output projections while keeping the high-capacity pretrained VAE core frozen. The architecture is illustrated in Figure~\ref{fig:semantic_vae_arch}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/semantic_vae_architecture.png}
    \caption{Architecture of the Semantic-Native VAE. The trainable Semantic Stem and Head (shown in blue) replace the original RGB input/output convolutions and interface directly with the frozen VAE encoder and decoder cores (shown in gray), bypassing the RGB bottleneck.}
    \label{fig:semantic_vae_arch}
\end{figure}

\paragraph{Why the Semantic Jump Is Needed: Removing the RGB Bottleneck.}
A naive approach to reuse the RGB VAE for semantic data is to map semantic IDs to a 3-channel color image and feed it into the pretrained encoder. This is problematic for two fundamental reasons:

\begin{enumerate}
    \item \textbf{Information bottleneck ($C \rightarrow 3$):} A one-hot semantic representation contains $C = 19$ mutually exclusive class channels. Mapping this to 3 channels necessarily collapses class separability: different classes may map to similar colors, RGB differences encode appearance rather than class identity, and the mapping is not invertible (many-to-one).

    \item \textbf{Domain mismatch of the learned RGB projection:} The first convolution of the pretrained VAE is trained to extract edges, textures, and colors from natural images. Semantic maps contain piecewise-constant regions and sharp class boundaries with fundamentally different statistics. Feeding such data through an RGB-trained input stem introduces a distribution shift at the earliest layer, which propagates through the entire encoder.
\end{enumerate}

For these reasons, we remove the RGB-specific $3 \rightarrow 128$ projection and replace it with a semantic-specific adapter. This replacement is what we refer to as the \emph{semantic jump}: it directly maps from the semantic domain into the internal feature space of the pretrained VAE core, bypassing the RGB bottleneck entirely. Concretely, the data path changes from:
\begin{align}
    \text{Original:} \quad &\mathbf{x}_{\text{rgb}} \xrightarrow{\text{Conv}^{3 \rightarrow 128}} \text{EncCore} \xrightarrow{} \mathbf{z} \xrightarrow{} \text{DecCore} \xrightarrow{\text{Conv}^{128 \rightarrow 3}} \hat{\mathbf{x}}_{\text{rgb}}, \nonumber \\
    \text{Semantic:} \quad &\mathbf{x}_{\text{oh}} \xrightarrow{\text{Stem}^{C \rightarrow 128}} \text{EncCore} \xrightarrow{} \mathbf{z} \xrightarrow{} \text{DecCore} \xrightarrow{\text{Head}^{128 \rightarrow C}} \hat{\mathbf{s}}.
    \label{eq:path_comparison}
\end{align}

\paragraph{Input Encoding.}
We first convert discrete semantic IDs into a one-hot tensor:
\begin{align}
    \mathbf{x}_{\text{oh}} = \text{OneHot}(\mathbf{y}) \in \{0, 1\}^{BT \times C \times H \times W}.
    \label{eq:onehot}
\end{align}
Pixels with the ignore index (255, used for unlabeled regions) are clamped to class 0 during encoding and excluded from the loss via a binary mask. This representation preserves semantic identity explicitly: each channel corresponds to exactly one class, and no information is lost through color encoding.

\paragraph{Semantic Input Adapter (Replacing the RGB Input Stem).}
In the original RGB VAE, the input projection performs:
$\mathbb{R}^{3 \times H \times W} \rightarrow \mathbb{R}^{128 \times H \times W}.$
We replace this with a trainable two-layer convolutional stem:
\begin{align}
    \text{Stem}(\mathbf{x}_{\text{oh}}) = \text{Conv}_{3 \times 3}^{64 \rightarrow 128}\!\left(\text{SiLU}\!\left(\text{GN}_8\!\left(\text{Conv}_{3 \times 3}^{C \rightarrow 64}(\mathbf{x}_{\text{oh}})\right)\right)\right),
    \label{eq:stem}
\end{align}
where $\text{GN}_8$ denotes Group Normalization~\cite{wu2018group} with 8 groups and $\text{SiLU}(x) = x \cdot \sigma(x)$ is the Sigmoid Linear Unit activation. Both convolutions use $3 \times 3$ kernels with padding~1 and are initialized with Kaiming normal initialization. This adapter serves two purposes: (i)~it removes the semantic compression bottleneck ($C \rightarrow 3$) by never projecting semantics into RGB, and (ii)~it matches the 128-channel interface expected by the pretrained encoder core.

\paragraph{Frozen VAE Encoder Core.}
The output of the semantic stem is fed into the \emph{frozen} pretrained VAE encoder core:
\begin{align}
    \mathbf{h}_{\text{enc}} = \text{EncCore}\!\left(\text{Stem}(\mathbf{x}_{\text{oh}})\right).
    \label{eq:enc_core}
\end{align}
The encoder core processes the 128-channel input through four downsampling blocks with residual connections and self-attention, a mid-block with self-attention, and a final convolution that produces Gaussian posterior parameters $(\boldsymbol{\mu},\, \log \boldsymbol{\sigma}^2) \in \mathbb{R}^{BT \times 8 \times H/8 \times W/8}$. We use \emph{deterministic encoding} for compatibility with the diffusion backbone:
\begin{align}
    \mathbf{z} = \boldsymbol{\mu} \in \mathbb{R}^{BT \times 4 \times H/8 \times W/8}.
    \label{eq:deterministic_latent}
\end{align}
Freezing the encoder core ensures that the resulting latent space remains aligned with the SVD latent geometry used in Stages~1 and~2.

\paragraph{Frozen VAE Decoder Core.}
The frozen decoder core maps latent codes back to 128-channel features through upsampling blocks with temporal (3D) convolutions and self-attention. Each frame is decoded independently with $T_{\text{dec}} = 1$ to produce features:
\begin{align}
    \mathbf{h}_{\text{dec}} = \text{DecCore}(\mathbf{z}) \in \mathbb{R}^{BT \times 128 \times H \times W}.
    \label{eq:dec_features}
\end{align}
Features are captured \emph{before} the original RGB output convolution using forward hooks. This is critical: it allows gradient flow through the frozen decoder back to the trainable stem during backpropagation, while completely bypassing the RGB output layer.

\paragraph{Semantic Output Adapter (Replacing the RGB Output Head).}
The pretrained decoder ends with a projection $\mathbb{R}^{128 \times H \times W} \rightarrow \mathbb{R}^{3 \times H \times W}$ that produces RGB pixel values. This is unsuitable for semantic reconstruction because semantics are \emph{categorical}, not continuous. We replace this with a trainable semantic classifier head:
\begin{align}
    \text{Head}(\mathbf{h}_{\text{dec}}) = \text{Conv}_{1 \times 1 \times 1}^{64 \rightarrow C}\!\left(\text{SiLU}\!\left(\text{GN}_8\!\left(\text{Conv}_{3 \times 3 \times 3}^{128 \rightarrow 64}(\mathbf{h}_{\text{dec}})\right)\right)\right),
    \label{eq:head}
\end{align}
producing per-pixel class logits $\hat{\mathbf{s}} \in \mathbb{R}^{B \times T \times C \times H \times W}$. The use of 3D convolutions ($3 \times 3 \times 3$ kernels in the first layer) allows the head to leverage temporal context for smoother predictions across frames, while the final $1 \times 1 \times 1$ convolution produces class-specific outputs.

\paragraph{Training Strategy: Freeze Core, Train Only Semantic Adapters.}
During training, all pretrained VAE core parameters are frozen:
\begin{align}
    \Theta_{\text{core}} = \{\Theta_{\text{EncCore}},\, \Theta_{\text{DecCore}}\} \quad \text{(frozen)},
    \label{eq:frozen_params}
\end{align}
and only the semantic adapters are optimized:
\begin{align}
    \Theta_{\text{sem}} = \{\Theta_{\text{Stem}},\, \Theta_{\text{Head}}\} \quad \text{(trainable)}.
    \label{eq:trainable_params}
\end{align}
This strategy provides two key benefits:
\begin{enumerate}
    \item \textbf{Latent space preservation:} The pretrained latent geometry used by SVD remains unchanged, ensuring compatibility with the diffusion models in Stages~1 and~2.
    \item \textbf{Parameter efficiency:} By training only $\sim$200K parameters (versus $\sim$84M frozen core parameters), the model avoids overfitting on limited semantic data and converges rapidly.
\end{enumerate}
Although the core is frozen, gradients flow \emph{through} it during backpropagation (it acts as a fixed differentiable mapping), but only $\Theta_{\text{sem}}$ receives parameter updates.

\paragraph{Training Objective: Cross-Entropy Loss.}
The task of the Semantic-Native VAE is to reconstruct a categorical semantic map. Each pixel corresponds to a discrete class index; therefore, this is a \emph{per-pixel classification} problem. The correct likelihood model is categorical:
\begin{align}
    p(y_{i,j} \mid \hat{\mathbf{s}}_{i,j}) = \text{Categorical}\!\left(\text{softmax}(\hat{\mathbf{s}}_{i,j})\right).
    \label{eq:categorical_likelihood}
\end{align}
Maximizing this likelihood leads directly to cross-entropy minimization:
\begin{align}
    \mathcal{L}_{\text{CE}} = -\sum_{b,t,i,j} m_{b,t,i,j} \cdot \log\!\left(\text{softmax}(\hat{\mathbf{s}}_{b,t,:,i,j})\right)_{y_{b,t,i,j}},
    \label{eq:ce_loss}
\end{align}
where $m$ is the ignore mask ($m = 0$ for pixels with ignore index, $m = 1$ otherwise).

Two design choices regarding the loss function merit explicit justification:

\begin{itemize}
    \item \textbf{Why not MSE or regression losses?} Using an $L_2$ loss on class indices would impose an artificial ordering (e.g., ``road = 0'' being numerically closer to ``sidewalk = 1'' than to ``car = 13''), which is semantically meaningless. Cross-entropy is invariant to label index permutations and correctly models discrete, unordered categories.

    \item \textbf{Why no KL regularization term?} A standard VAE objective includes a KL divergence term $\mathcal{L}_{\text{VAE}} = \mathcal{L}_{\text{recon}} + \beta\, D_{\text{KL}}(q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z}))$ to enforce a latent prior. In our case, the encoder core is \emph{frozen} and already trained to produce latents compatible with the diffusion prior. Introducing an additional KL term would either be ineffective (since the core parameters cannot be updated) or would distort the learned semantic adapters away from the pretrained latent geometry. Therefore, we use deterministic latents $\mathbf{z} = \boldsymbol{\mu}$ and optimize only reconstruction via cross-entropy.
\end{itemize}

This design makes the Semantic-Native VAE behave as a \emph{semantic interface to a fixed latent manifold}, rather than a fully retrained generative autoencoder.

\paragraph{Parameter Count.}
The Semantic Stem contains approximately 10,000 parameters and the Semantic Head approximately 190,000 parameters, for a total of $\sim$200,000 trainable parameters. The frozen VAE core contains $\sim$84 million parameters. This parameter-efficient design enables fast convergence with limited training data and minimal risk of overfitting.

\subsection{Stage~1: Semantic Video Prediction}
\label{subsec:stage1}

Stage~1 adapts the SVD backbone to predict semantic video sequences. The model receives:
\begin{itemize}
    \item $\hat{\mathbf{s}}_t \in \mathbb{R}^{T \times 4 \times h \times w}$: Noisy latent representation of the semantic video (being denoised),
    \item $\mathbf{z}^{(0)} \in \mathbb{R}^{1 \times 4 \times h \times w}$: Latent encoding of the initial RGB frame (via the RGB VAE),
    \item $\mathbf{c}^{(0)}$: CLIP encoding of the initial frame (for cross-attention conditioning),
\end{itemize}
where $h = H/8 = 24$ and $w = W/8 = 88$ at the training resolution of $192 \times 704$.

The initial frame latent is padded (repeated) along the temporal dimension and concatenated with the noisy semantic latents along the channel dimension, following the SVD conditioning mechanism. The UNet predicts the noise $\boldsymbol{\epsilon}_\theta(\hat{\mathbf{s}}_t, t, \mathbf{z}^{(0)}_{\text{pad}}, \mathbf{c}^{(0)})$ and is trained with the standard diffusion objective (Equation~\ref{eq:ddpm_loss}).

The encoding of semantic IDs into latent space uses the trained Semantic VAE (frozen during Stage~1 training): semantic IDs $\rightarrow$ Semantic VAE encoder $\rightarrow$ semantic latents $\mathbf{s}$.

\subsection{Stage~2: Semantic-to-Video Generation}
\label{subsec:stage2}

Stage~2 generates photorealistic RGB videos conditioned on semantic maps using a ControlNet architecture~\cite{zhang2023adding} adapted to video. The model consists of:

\begin{itemize}
    \item \textbf{SVD Backbone (frozen):} Receives the initial frame latent $\mathbf{z}^{(0)}_{\text{pad}}$ concatenated with noisy RGB video latents $\hat{\mathbf{z}}_t$, plus CLIP conditioning $\mathbf{c}^{(0)}$ via cross-attention.
    
    \item \textbf{ControlNet (trainable):} A copy of the SVD encoder blocks that receives the same noisy latents plus the semantic latents $\mathbf{s}$ (encoded by the Semantic VAE). The ControlNet output is injected into the SVD decoder via zero-initialized convolutions.
\end{itemize}

The training objective for Stage~2 is:
\begin{align}
    \mathcal{L}_{\text{Stage2}} = \mathbb{E}_{t, \mathbf{z}_0, \boldsymbol{\epsilon}}\!\left[\left\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\theta,\xi}(\hat{\mathbf{z}}_t, t, \mathbf{z}^{(0)}_{\text{pad}}, \mathbf{c}^{(0)}, \mathbf{s})\right\|^2\right],
    \label{eq:stage2_loss}
\end{align}
where $\theta$ (SVD UNet) is frozen and only $\xi$ (ControlNet) is updated.

During training, ground-truth semantic labels are used as conditioning (encoded via the Semantic VAE). This allows Stage~2 to be trained in parallel with Stage~1, since it does not depend on Stage~1 predictions. During inference, the semantic maps predicted by Stage~1 are used instead.

% -----------------------------------------------------------------
\section{Loss Functions}
\label{sec:losses}

\subsection{Semantic VAE Losses}
\label{subsec:vae_losses}

As established in Section~\ref{subsec:semantic_native_vae}, the Semantic-Native VAE is trained with cross-entropy as the primary objective (Equation~\ref{eq:ce_loss}). In practice, we augment this base loss with two refinements: boundary weighting and Dice loss.

\paragraph{Boundary-Weighted Cross-Entropy.}
Standard cross-entropy treats all pixels equally, which causes the model to focus on large homogeneous regions (e.g., road, sky) while neglecting thin structures (e.g., poles, traffic signs) and class boundaries. To address this, we introduce pixel-wise weights based on a boundary mask:
\begin{align}
    \mathcal{L}_{\text{BCE}} = \frac{\sum_{i \in \mathcal{V}} w_i \cdot \ell_{\text{CE}}(\hat{\mathbf{s}}_i, y_i)}{\sum_{i \in \mathcal{V}} w_i}, \quad w_i = 1 + \alpha \cdot b_i,
    \label{eq:bce_loss}
\end{align}
where $\ell_{\text{CE}}(\hat{\mathbf{s}}_i, y_i) = -\log\!\left(\text{softmax}(\hat{\mathbf{s}}_i)\right)_{y_i}$ is the per-pixel cross-entropy, $b_i \in \{0, 1\}$ is the boundary mask, $\alpha$ is the boundary emphasis factor, and $\mathcal{V}$ is the set of valid (non-ignored) pixels.

The boundary mask is computed efficiently via max-min pooling with a $3 \times 3$ kernel:
\begin{align}
    b_i = \begin{cases} 1 & \text{if } \max_{j \in \mathcal{N}(i)} y_j \neq \min_{j \in \mathcal{N}(i)} y_j, \\ 0 & \text{otherwise,}\end{cases}
    \label{eq:boundary_mask}
\end{align}
where $\mathcal{N}(i)$ is the $3 \times 3$ neighborhood of pixel $i$. This identifies pixels at class transitions without requiring explicit edge detection. With $\alpha = 4.0$, boundary pixels receive $5\times$ the loss weight of interior pixels, encouraging the model to preserve sharp class boundaries.

\paragraph{Dice Loss.}
To address class imbalance---vegetation may occupy over $1{,}000\times$ more pixels than traffic signs---we employ the soft Dice loss~\cite{milletari2016vnet}:
\begin{align}
    \mathcal{L}_{\text{Dice}} = 1 - \frac{1}{C} \sum_{c=1}^{C} \frac{2 \sum_i p_{i,c} \cdot t_{i,c} + \epsilon}{\sum_i p_{i,c} + \sum_i t_{i,c} + \epsilon},
    \label{eq:dice_loss}
\end{align}
where $p_{i,c} = \text{softmax}(\hat{\mathbf{s}}_i)_c$ is the predicted probability for class $c$ at pixel $i$, $t_{i,c}$ is the one-hot ground truth, and $\epsilon = 1$ is a smoothing constant. The Dice loss computes a per-class overlap measure and averages across classes, thereby treating all classes equally regardless of their spatial prevalence.

\paragraph{Combined Loss.}
The total loss for Semantic VAE training is:
\begin{align}
    \mathcal{L}_{\text{VAE}} = \mathcal{L}_{\text{BCE}} + \lambda_{\text{Dice}} \cdot \mathcal{L}_{\text{Dice}},
    \label{eq:total_vae_loss}
\end{align}
where $\lambda_{\text{Dice}} = 0.5$ balances the two objectives. The ablation of these loss components is presented in Section~\ref{subsec:loss_ablation}.

\subsection{Diffusion Training Losses}
\label{subsec:diffusion_losses}

Both Stage~1 and Stage~2 use the standard diffusion denoising objective with EDM noise scheduling~\cite{karras2022elucidating}. The loss is a weighted mean-squared error between the predicted and actual noise:
\begin{align}
    \mathcal{L}_{\text{diffusion}} = \mathbb{E}_{t \sim p(t), \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}\!\left[w(t) \cdot \left\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \mathbf{c})\right\|^2\right],
    \label{eq:diffusion_loss}
\end{align}
where $w(t)$ is a time-dependent weighting function defined by the EDM schedule and $\mathbf{c}$ represents the conditioning inputs (initial frame, CLIP embedding, and---for Stage~2---semantic latents).
