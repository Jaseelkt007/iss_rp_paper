% =================================================================
% CHAPTER 3: METHODOLOGY
% =================================================================
\chapter{Methodology}
\label{ch:methodology}

This chapter describes the dataset, evaluation metrics, the proposed model architectures, and the loss functions employed in this work.

% -----------------------------------------------------------------
\section{Dataset: KITTI-360}
\label{sec:dataset}

\subsection{Overview}
\label{subsec:dataset_overview}

The KITTI-360 dataset~\cite{liao2022kitti360} is a large-scale outdoor dataset for autonomous driving research, extending the original KITTI benchmark~\cite{geiger2012are} with richer annotations including dense semantic and instance segmentation, 3D bounding boxes, and accumulated point clouds. The dataset was captured in suburban areas of Karlsruhe, Germany, using a Volkswagen station wagon equipped with:
\begin{itemize}
    \item Two high-resolution color cameras (stereo),
    \item A Velodyne HDL-64E 3D laser scanner,
    \item An OXTS RT3003 GPS/IMU localization unit.
\end{itemize}

For this work, we use the front-facing camera (camera~00) images and corresponding semantic segmentation annotations. The dataset is split into 9 driving sequences covering a total of approximately 73,000 frames.

\subsection{Semantic Annotations}
\label{subsec:semantic_annotations}

The semantic annotations use 19 training classes consistent with the Cityscapes label definition~\cite{cordts2016cityscapes}. Raw KITTI-360 label IDs are remapped to continuous training IDs (0--18) using the official \texttt{kitti360scripts} library. Table~\ref{tab:kitti360_classes} lists the semantic classes used.

\begin{table}[t]
    \centering
    \caption{KITTI-360 semantic classes with training IDs used in this work}
    \label{tab:kitti360_classes}
    \begin{tabular}{clcl}
        \toprule
        \textbf{Train ID} & \textbf{Class} & \textbf{Train ID} & \textbf{Class} \\
        \midrule
        0  & Road         & 10 & Sky \\
        1  & Sidewalk     & 11 & Person \\
        2  & Building     & 12 & Rider \\
        3  & Wall         & 13 & Car \\
        4  & Fence        & 14 & Truck \\
        5  & Pole         & 15 & Bus \\
        6  & Traffic Light & 16 & Train \\
        7  & Traffic Sign & 17 & Motorcycle \\
        8  & Vegetation   & 18 & Bicycle \\
        9  & Terrain      &    & \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Data Preparation}
\label{subsec:data_preparation}

The official train/validation split files (\texttt{2013\_05\_28\_drive\_train\_frames.txt} and \texttt{2013\_05\_28\_drive\_val\_frames.txt}) define paired RGB and semantic paths. Each pair specifies an RGB image from \texttt{data\_2d\_raw/} and its corresponding semantic label map from \texttt{data\_2d\_semantics/train/}.

Key preprocessing steps:
\begin{itemize}
    \item \textbf{Resolution:} Images are resized from the original $376 \times 1408$ to $192 \times 704$ using bilinear interpolation for RGB and nearest-neighbor interpolation for semantic maps (to preserve discrete labels).
    \item \textbf{Clip Formation:} Consecutive frames are grouped into clips of $T=25$ frames for diffusion training and $T=4$ frames for Semantic VAE training.
    \item \textbf{Normalization:} RGB images are normalized to $[-1, 1]$; semantic labels remain as integer IDs.
\end{itemize}

The training set contains approximately 49,000 frame pairs. For Semantic VAE training, a subset of 500 clips (2,000 frames at $T=4$) is used due to the small number of trainable parameters, with 200 clips reserved for validation.

% -----------------------------------------------------------------
\section{Evaluation Metrics}
\label{sec:metrics}

\subsection{Fr\'echet Inception Distance (FID)}
\label{subsec:fid}

The Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} measures the similarity between the distributions of generated and real images in the feature space of an Inception-v3 network~\cite{szegedy2016rethinking}. Given multivariate Gaussian fits $\mathcal{N}(\boldsymbol{\mu}_r, \boldsymbol{\Sigma}_r)$ and $\mathcal{N}(\boldsymbol{\mu}_g, \boldsymbol{\Sigma}_g)$ to the real and generated feature distributions respectively:
\begin{align}
    \text{FID} = \|\boldsymbol{\mu}_r - \boldsymbol{\mu}_g\|^2 + \text{Tr}\!\left(\boldsymbol{\Sigma}_r + \boldsymbol{\Sigma}_g - 2\left(\boldsymbol{\Sigma}_r \boldsymbol{\Sigma}_g\right)^{1/2}\right).
    \label{eq:fid}
\end{align}
Lower FID scores indicate higher similarity between generated and real image distributions. FID is computed on individual frames extracted from generated and ground-truth videos.

\subsection{Fr\'echet Video Distance (FVD)}
\label{subsec:fvd}

The Fr\'echet Video Distance (FVD)~\cite{unterthiner2019fvd} extends FID to the video domain by using features from an Inflated 3D ConvNet (I3D)~\cite{carreira2017quo} pretrained on Kinetics-400. The I3D network captures both spatial appearance and temporal dynamics:
\begin{align}
    \text{FVD} = \|\boldsymbol{\mu}_r^{\text{I3D}} - \boldsymbol{\mu}_g^{\text{I3D}}\|^2 + \text{Tr}\!\left(\boldsymbol{\Sigma}_r^{\text{I3D}} + \boldsymbol{\Sigma}_g^{\text{I3D}} - 2\left(\boldsymbol{\Sigma}_r^{\text{I3D}} \boldsymbol{\Sigma}_g^{\text{I3D}}\right)^{1/2}\right).
    \label{eq:fvd}
\end{align}
FVD evaluates both the visual quality and temporal coherence of generated videos. Lower values are better.

\subsection{Mean Intersection-over-Union (mIoU)}
\label{subsec:miou}

For evaluating the Semantic VAE, the Mean Intersection-over-Union is the primary metric. For a given class $c$:
\begin{align}
    \text{IoU}_c = \frac{|\mathcal{P}_c \cap \mathcal{T}_c|}{|\mathcal{P}_c \cup \mathcal{T}_c|} = \frac{\text{TP}_c}{\text{TP}_c + \text{FP}_c + \text{FN}_c},
    \label{eq:iou}
\end{align}
where $\mathcal{P}_c$ and $\mathcal{T}_c$ are the sets of pixels predicted and labeled as class $c$, respectively. The mean IoU averages over all valid classes:
\begin{align}
    \text{mIoU} = \frac{1}{|\mathcal{C}_{\text{valid}}|} \sum_{c \in \mathcal{C}_{\text{valid}}} \text{IoU}_c.
    \label{eq:miou}
\end{align}
Unlike pixel accuracy, mIoU treats all classes equally regardless of their spatial prevalence, making it sensitive to performance on rare classes such as poles and traffic signs.

\subsection{Additional Video Quality Metrics}
\label{subsec:additional_metrics}

In addition to FID and FVD, we report several complementary metrics:

\begin{itemize}
    \item \textbf{LPIPS} (Learned Perceptual Image Patch Similarity)~\cite{zhang2018unreasonable}: Measures perceptual distance between image pairs using deep features. Lower is better.
    \item \textbf{SSIM} (Structural Similarity Index): Evaluates structural similarity between generated and ground-truth frames, considering luminance, contrast, and structure. Higher is better, with a maximum of 1.0.
    \item \textbf{PSNR} (Peak Signal-to-Noise Ratio): Measures pixel-level reconstruction quality in decibels. Higher values indicate less distortion.
\end{itemize}

% -----------------------------------------------------------------
\section{Model Architecture}
\label{sec:architecture}

\subsection{Overview of the Two-Stage Pipeline}
\label{subsec:pipeline_overview}

The proposed pipeline, illustrated in Figure~\ref{fig:pipeline_overview}, consists of three components:

\begin{enumerate}
    \item \textbf{Semantic VAE} (pretrained, frozen during diffusion training): Encodes semantic label maps into and decodes them from the latent space shared with the RGB VAE.
    \item \textbf{Stage~1 --- Semantic Prediction Model:} A diffusion model that predicts future semantic video latents given an initial RGB frame.
    \item \textbf{Stage~2 --- Semantic-to-Video Model:} A ControlNet-conditioned diffusion model that generates RGB video latents conditioned on semantic video latents.
\end{enumerate}

\begin{figure}[t]
    \centering
    % TODO: Replace with actual pipeline diagram
    \includegraphics[width=0.9\textwidth]{images/ctrl_v_pipleline.png}
    \caption{Overview of the proposed two-stage pipeline for semantically controlled video generation. The Semantic VAE bridges discrete semantic maps and continuous latent space. Stage~1 predicts future semantic sequences. Stage~2 generates photorealistic RGB video conditioned on the predicted semantics.}
    \label{fig:pipeline_overview}
\end{figure}

\subsection{Semantic-Native VAE}
\label{subsec:semantic_native_vae}

The pretrained Stable Video Diffusion (SVD) VAE was trained on natural RGB images and therefore assumes an input tensor $\mathbf{x}_{\text{rgb}} \in \mathbb{R}^{B \times T \times 3 \times H \times W}$. Internally, the VAE begins with a learned projection that converts 3 RGB channels into the feature width expected by the encoder backbone (128 channels), and ends with a symmetric projection from 128-channel decoder features back to 3 RGB channels. However, semantic label maps are \emph{discrete} variables
\begin{align}
    \mathbf{y} \in \{0, \ldots, C-1\}^{B \times T \times H \times W}, \quad C = 19,
    \label{eq:semantic_input}
\end{align}
and do not admit a meaningful representation in 3 channels without information loss. To bridge this mismatch, we introduce a \emph{Semantic-Native VAE} that replaces only the RGB-specific input and output projections while keeping the high-capacity pretrained VAE core frozen. The architecture is illustrated in Figure~\ref{fig:semantic_vae_arch}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/semantic_vae_architecture.png}
    \caption{Architecture of the Semantic-Native VAE. The trainable Semantic Stem and Head (shown in blue) replace the original RGB input/output convolutions and interface directly with the frozen VAE encoder and decoder cores (shown in gray), bypassing the RGB bottleneck.}
    \label{fig:semantic_vae_arch}
\end{figure}

\paragraph{Why the Semantic Jump Is Needed: Removing the RGB Bottleneck.}
A naive approach to reuse the RGB VAE for semantic data is to map semantic IDs to a 3-channel color image and feed it into the pretrained encoder. This is problematic for two fundamental reasons:

\begin{enumerate}
    \item \textbf{Information bottleneck ($C \rightarrow 3$):} A one-hot semantic representation contains $C = 19$ mutually exclusive class channels. Mapping this to 3 channels necessarily collapses class separability: different classes may map to similar colors, RGB differences encode appearance rather than class identity, and the mapping is not invertible (many-to-one).

    \item \textbf{Domain mismatch of the learned RGB projection:} The first convolution of the pretrained VAE is trained to extract edges, textures, and colors from natural images. Semantic maps contain piecewise-constant regions and sharp class boundaries with fundamentally different statistics. Feeding such data through an RGB-trained input stem introduces a distribution shift at the earliest layer, which propagates through the entire encoder.
\end{enumerate}

For these reasons, we remove the RGB-specific $3 \rightarrow 128$ projection and replace it with a semantic-specific adapter. This replacement is what we refer to as the \emph{semantic jump}: it directly maps from the semantic domain into the internal feature space of the pretrained VAE core, bypassing the RGB bottleneck entirely. Concretely, the data path changes from:
\begin{align}
    \text{Original:} \quad &\mathbf{x}_{\text{rgb}} \xrightarrow{\text{Conv}^{3 \rightarrow 128}} \text{EncCore} \xrightarrow{} \mathbf{z} \xrightarrow{} \text{DecCore} \xrightarrow{\text{Conv}^{128 \rightarrow 3}} \hat{\mathbf{x}}_{\text{rgb}}, \nonumber \\
    \text{Semantic:} \quad &\mathbf{x}_{\text{oh}} \xrightarrow{\text{Stem}^{C \rightarrow 128}} \text{EncCore} \xrightarrow{} \mathbf{z} \xrightarrow{} \text{DecCore} \xrightarrow{\text{Head}^{128 \rightarrow C}} \hat{\mathbf{s}}.
    \label{eq:path_comparison}
\end{align}

\paragraph{Input Encoding.}
We first convert discrete semantic IDs into a one-hot tensor:
\begin{align}
    \mathbf{x}_{\text{oh}} = \text{OneHot}(\mathbf{y}) \in \{0, 1\}^{BT \times C \times H \times W}.
    \label{eq:onehot}
\end{align}
Pixels with the ignore index (255, used for unlabeled regions) are clamped to class 0 during encoding and excluded from the loss via a binary mask. This representation preserves semantic identity explicitly: each channel corresponds to exactly one class, and no information is lost through color encoding.

\paragraph{Semantic Input Adapter (Replacing the RGB Input Stem).}
In the original RGB VAE, the input projection performs:
$\mathbb{R}^{3 \times H \times W} \rightarrow \mathbb{R}^{128 \times H \times W}.$
We replace this with a trainable two-layer convolutional stem:
\begin{align}
    \text{Stem}(\mathbf{x}_{\text{oh}}) = \text{Conv}_{3 \times 3}^{64 \rightarrow 128}\!\left(\text{SiLU}\!\left(\text{GN}_8\!\left(\text{Conv}_{3 \times 3}^{C \rightarrow 64}(\mathbf{x}_{\text{oh}})\right)\right)\right),
    \label{eq:stem}
\end{align}
where $\text{GN}_8$ denotes Group Normalization~\cite{wu2018group} with 8 groups and $\text{SiLU}(x) = x \cdot \sigma(x)$ is the Sigmoid Linear Unit activation. Both convolutions use $3 \times 3$ kernels with padding~1 and are initialized with Kaiming normal initialization. This adapter serves two purposes: (i)~it removes the semantic compression bottleneck ($C \rightarrow 3$) by never projecting semantics into RGB, and (ii)~it matches the 128-channel interface expected by the pretrained encoder core.

\paragraph{Frozen VAE Encoder Core.}
The output of the semantic stem is fed into the \emph{frozen} pretrained VAE encoder core:
\begin{align}
    \mathbf{h}_{\text{enc}} = \text{EncCore}\!\left(\text{Stem}(\mathbf{x}_{\text{oh}})\right).
    \label{eq:enc_core}
\end{align}
The encoder core processes the 128-channel input through four downsampling blocks with residual connections and self-attention, a mid-block with self-attention, and a final convolution that produces Gaussian posterior parameters $(\boldsymbol{\mu},\, \log \boldsymbol{\sigma}^2) \in \mathbb{R}^{BT \times 8 \times H/8 \times W/8}$. We use \emph{deterministic encoding} for compatibility with the diffusion backbone:
\begin{align}
    \mathbf{z} = \boldsymbol{\mu} \in \mathbb{R}^{BT \times 4 \times H/8 \times W/8}.
    \label{eq:deterministic_latent}
\end{align}
Freezing the encoder core ensures that the resulting latent space remains aligned with the SVD latent geometry used in Stages~1 and~2.

\paragraph{Frozen VAE Decoder Core.}
The frozen decoder core maps latent codes back to 128-channel features through upsampling blocks with temporal (3D) convolutions and self-attention. Each frame is decoded independently with $T_{\text{dec}} = 1$ to produce features:
\begin{align}
    \mathbf{h}_{\text{dec}} = \text{DecCore}(\mathbf{z}) \in \mathbb{R}^{BT \times 128 \times H \times W}.
    \label{eq:dec_features}
\end{align}
Features are captured \emph{before} the original RGB output convolution using forward hooks. This is critical: it allows gradient flow through the frozen decoder back to the trainable stem during backpropagation, while completely bypassing the RGB output layer.

\paragraph{Semantic Output Adapter (Replacing the RGB Output Head).}
The pretrained decoder ends with a projection $\mathbb{R}^{128 \times H \times W} \rightarrow \mathbb{R}^{3 \times H \times W}$ that produces RGB pixel values. This is unsuitable for semantic reconstruction because semantics are \emph{categorical}, not continuous. We replace this with a trainable semantic classifier head:
\begin{align}
    \text{Head}(\mathbf{h}_{\text{dec}}) = \text{Conv}_{1 \times 1 \times 1}^{64 \rightarrow C}\!\left(\text{SiLU}\!\left(\text{GN}_8\!\left(\text{Conv}_{3 \times 3 \times 3}^{128 \rightarrow 64}(\mathbf{h}_{\text{dec}})\right)\right)\right),
    \label{eq:head}
\end{align}
producing per-pixel class logits $\hat{\mathbf{s}} \in \mathbb{R}^{B \times T \times C \times H \times W}$. The use of 3D convolutions ($3 \times 3 \times 3$ kernels in the first layer) allows the head to leverage temporal context for smoother predictions across frames, while the final $1 \times 1 \times 1$ convolution produces class-specific outputs.

\paragraph{Training Strategy: Freeze Core, Train Only Semantic Adapters.}
During training, all pretrained VAE core parameters are frozen:
\begin{align}
    \Theta_{\text{core}} = \{\Theta_{\text{EncCore}},\, \Theta_{\text{DecCore}}\} \quad \text{(frozen)},
    \label{eq:frozen_params}
\end{align}
and only the semantic adapters are optimized:
\begin{align}
    \Theta_{\text{sem}} = \{\Theta_{\text{Stem}},\, \Theta_{\text{Head}}\} \quad \text{(trainable)}.
    \label{eq:trainable_params}
\end{align}
This strategy provides two key benefits:
\begin{enumerate}
    \item \textbf{Latent space preservation:} The pretrained latent geometry used by SVD remains unchanged, ensuring compatibility with the diffusion models in Stages~1 and~2.
    \item \textbf{Parameter efficiency:} By training only $\sim$200K parameters (versus $\sim$84M frozen core parameters), the model avoids overfitting on limited semantic data and converges rapidly.
\end{enumerate}
Although the core is frozen, gradients flow \emph{through} it during backpropagation (it acts as a fixed differentiable mapping), but only $\Theta_{\text{sem}}$ receives parameter updates.

\paragraph{Training Objective: Cross-Entropy Loss.}
The task of the Semantic-Native VAE is to reconstruct a categorical semantic map. Each pixel corresponds to a discrete class index; therefore, this is a \emph{per-pixel classification} problem. The correct likelihood model is categorical:
\begin{align}
    p(y_{i,j} \mid \hat{\mathbf{s}}_{i,j}) = \text{Categorical}\!\left(\text{softmax}(\hat{\mathbf{s}}_{i,j})\right).
    \label{eq:categorical_likelihood}
\end{align}
Maximizing this likelihood leads directly to cross-entropy minimization:
\begin{align}
    \mathcal{L}_{\text{CE}} = -\sum_{b,t,i,j} m_{b,t,i,j} \cdot \log\!\left(\text{softmax}(\hat{\mathbf{s}}_{b,t,:,i,j})\right)_{y_{b,t,i,j}},
    \label{eq:ce_loss}
\end{align}
where $m$ is the ignore mask ($m = 0$ for pixels with ignore index, $m = 1$ otherwise).

Two design choices regarding the loss function merit explicit justification:

\begin{itemize}
    \item \textbf{Why not MSE or regression losses?} Using an $L_2$ loss on class indices would impose an artificial ordering (e.g., ``road = 0'' being numerically closer to ``sidewalk = 1'' than to ``car = 13''), which is semantically meaningless. Cross-entropy is invariant to label index permutations and correctly models discrete, unordered categories.

    \item \textbf{Why no KL regularization term?} A standard VAE objective includes a KL divergence term $\mathcal{L}_{\text{VAE}} = \mathcal{L}_{\text{recon}} + \beta\, D_{\text{KL}}(q(\mathbf{z} \mid \mathbf{x}) \| p(\mathbf{z}))$ to enforce a latent prior. In our case, the encoder core is \emph{frozen} and already trained to produce latents compatible with the diffusion prior. Introducing an additional KL term would either be ineffective (since the core parameters cannot be updated) or would distort the learned semantic adapters away from the pretrained latent geometry. Therefore, we use deterministic latents $\mathbf{z} = \boldsymbol{\mu}$ and optimize only reconstruction via cross-entropy.
\end{itemize}

This design makes the Semantic-Native VAE behave as a \emph{semantic interface to a fixed latent manifold}, rather than a fully retrained generative autoencoder.

\paragraph{Parameter Count.}
The Semantic Stem contains approximately 10,000 parameters and the Semantic Head approximately 190,000 parameters, for a total of $\sim$200,000 trainable parameters. The frozen VAE core contains $\sim$84 million parameters. This parameter-efficient design enables fast convergence with limited training data and minimal risk of overfitting.

\subsection{Stage~1: Semantic Video Prediction}
\label{subsec:stage1}

Stage~1 is responsible for predicting a temporally coherent sequence of semantic segmentation maps conditioned on a single RGB frame. This stage adapts the Stable Video Diffusion (SVD) backbone to operate in the latent space of semantic segmentation rather than RGB space. The model learns to generate plausible future scene layouts---road geometry, object positions, and semantic structure---from a single initial observation.

\paragraph{Semantic Latent Encoding.}
Ground-truth semantic segmentation maps are converted into continuous latent representations using the Semantic-Native VAE described in Section~\ref{subsec:semantic_native_vae}. The Semantic VAE is \emph{frozen} during Stage~1 training:
\begin{align}
    \mathbf{s} = \text{SemVAE}_{\text{enc}}(\mathbf{y}) \in \mathbb{R}^{B \times T \times 4 \times h \times w},
    \label{eq:semantic_latent_enc}
\end{align}
where $\mathbf{y}$ denotes the semantic ID maps and $h = H/8 = 24$, $w = W/8 = 88$ at the training resolution of $192 \times 704$. This encoding produces a compact 4-channel latent representation that is compatible with the SVD latent space and amenable to diffusion modeling.

\paragraph{Input Representation.}
The model operates entirely in latent space. Three conditioning signals are prepared:

\begin{enumerate}
    \item \textbf{Noisy semantic latents} $\hat{\mathbf{s}}_t \in \mathbb{R}^{B \times T \times 4 \times h \times w}$: The diffusion target. Gaussian noise is added to the clean semantic latents $\mathbf{s}_0$ according to the EDM noise schedule:
    \begin{align}
        \hat{\mathbf{s}}_t = \alpha_t\, \mathbf{s}_0 + \sigma_t\, \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
        \label{eq:noisy_semantic}
    \end{align}
    The noisy latents are then scaled by $\hat{\mathbf{s}}_t / \sqrt{\sigma^2 + 1}$ before being passed to the UNet, following the EDM preconditioning (Section~\ref{subsec:edm}).

    \item \textbf{Initial RGB frame latent} $\mathbf{z}^{(0)} \in \mathbb{R}^{B \times 4 \times h \times w}$: The first frame of the driving video, encoded using the \emph{pretrained RGB VAE} (not the Semantic VAE):
    \begin{align}
        \mathbf{z}^{(0)} = \text{VAE}_{\text{enc}}(\mathbf{f}^{(0)}),
        \label{eq:initial_frame_latent}
    \end{align}
    where $\mathbf{f}^{(0)} \in \mathbb{R}^{B \times 3 \times H \times W}$ is the initial RGB frame. This latent provides spatial appearance information that anchors the predicted semantic sequence.

    \item \textbf{CLIP image embedding} $\mathbf{c}^{(0)} \in \mathbb{R}^{B \times 1 \times d_c}$: The initial RGB frame is resized to $224 \times 224$, normalized according to CLIP preprocessing, and passed through a frozen CLIP vision encoder (\texttt{CLIPVisionModelWithProjection}):
    \begin{align}
        \mathbf{c}^{(0)} = \text{CLIP}_{\text{enc}}(\mathbf{f}^{(0)}) \in \mathbb{R}^{B \times 1 \times d_c},
        \label{eq:clip_embedding}
    \end{align}
    where $d_c = 1024$ is the CLIP embedding dimension. This embedding captures high-level semantic and scene context from the initial frame.
\end{enumerate}

\paragraph{Conditioning Mechanism.}
Stage~1 follows the conditioning design of SVD, which uses two distinct pathways to inject conditioning information:

\textit{Channel concatenation conditioning.} The initial frame latent $\mathbf{z}^{(0)}$ is temporally replicated to match the video length and concatenated along the channel dimension with the noisy semantic latents. Additionally, in the trajectory-conditioned variant, the first and last semantic frame latents are preserved in the conditioning tensor to provide boundary constraints:
\begin{align}
    \mathbf{z}^{(0)}_{\text{pad}} &= \text{Repeat}(\mathbf{z}^{(0)}, T) \in \mathbb{R}^{B \times T \times 4 \times h \times w}, \label{eq:frame_padding} \\
    \mathbf{g} &= \left[\mathbf{s}^{(0)},\, \ldots,\, \mathbf{s}^{(k-1)},\, \mathbf{z}^{(0)},\, \ldots,\, \mathbf{z}^{(0)},\, \mathbf{s}^{(T-1)}\right] \in \mathbb{R}^{B \times T \times 4 \times h \times w}, \label{eq:cond_latents_stage1}
\end{align}
where $k$ denotes the number of initial conditioning semantic frames (\texttt{num\_cond\_bbox\_frames}). The conditioning tensor $\mathbf{g}$ thus contains: (i)~the first $k$ semantic frame latents, (ii)~the initial RGB frame latent repeated for the intermediate frames, and (iii)~the last semantic frame latent. This design provides both spatial anchoring (via the RGB frame) and temporal boundary constraints (via the first and last semantic frames).

The final UNet input is formed by concatenating the scaled noisy semantic latents with the conditioning tensor along the channel dimension:
\begin{align}
    \mathbf{x}_{\text{in}} = \left[\hat{\mathbf{s}}_t / \sqrt{\sigma^2 + 1} \;\|\; \mathbf{g}\right] \in \mathbb{R}^{B \times T \times 8 \times h \times w},
    \label{eq:unet_input_stage1}
\end{align}
where $\|$ denotes channel-wise concatenation. The UNet thus receives an 8-channel input per frame: 4 channels from the noisy semantic latents and 4 channels from the conditioning signal.

\textit{Cross-attention conditioning.} The CLIP embedding $\mathbf{c}^{(0)}$ is injected via cross-attention layers inside the UNet. Specifically, the encoder hidden states are repeated across all $T$ frames:
\begin{align}
    \mathbf{c}_{\text{rep}} = \text{RepeatInterleave}(\mathbf{c}^{(0)}, T) \in \mathbb{R}^{(B \cdot T) \times 1 \times d_c},
    \label{eq:clip_repeat}
\end{align}
and used as key and value inputs in the cross-attention layers of the \texttt{CrossAttnDownBlockSpatioTemporal} and \texttt{CrossAttnUpBlockSpatioTemporal} blocks. This provides global semantic context from the initial frame that guides the generation of all subsequent frames without requiring explicit per-frame conditioning.

\paragraph{Additional Time Embeddings.}
SVD conditions on auxiliary metadata through sinusoidal time embeddings. Three quantities are encoded and added to the diffusion timestep embedding:
\begin{itemize}
    \item \textbf{Frame rate} ($\text{fps} - 1$): Encodes the temporal sampling rate of the training data,
    \item \textbf{Motion bucket ID} ($= 127$): Controls the expected amount of motion in the generated video,
    \item \textbf{Noise augmentation strength}: Indicates the noise level applied to the conditioning frame.
\end{itemize}
These are projected through sinusoidal embeddings and added element-wise to the diffusion time embedding via a learned linear projection (\texttt{add\_embedding}), ensuring that the UNet is aware of the temporal and noise characteristics of the input.

\paragraph{Diffusion Objective.}
The UNet predicts the noise added to the semantic latents. Following the EDM formulation~\cite{karras2022elucidating}, the model output is combined with the noisy input using learned skip connections:
\begin{align}
    \hat{\mathbf{s}}_{\text{denoised}} = c_{\text{out}}(\sigma) \cdot \boldsymbol{\epsilon}_\theta(\mathbf{x}_{\text{in}}, t, \mathbf{c}_{\text{rep}}, \mathbf{a}) + c_{\text{skip}}(\sigma) \cdot \hat{\mathbf{s}}_t,
    \label{eq:edm_denoised_stage1}
\end{align}
where $c_{\text{out}}(\sigma) = -\sigma / \sqrt{\sigma^2 + 1}$, $c_{\text{skip}}(\sigma) = 1 / (\sigma^2 + 1)$, and $\mathbf{a}$ denotes the added time IDs. The training loss is a sigma-weighted MSE:
\begin{align}
    \mathcal{L}_{\text{Stage1}} = \mathbb{E}_{t,\, \mathbf{s}_0,\, \boldsymbol{\epsilon}}\!\left[w(\sigma) \cdot \left\|\hat{\mathbf{s}}_{\text{denoised}} - \mathbf{s}_0\right\|^2\right], \quad w(\sigma) = \frac{\sigma^2 + 1}{\sigma^2},
    \label{eq:stage1_loss}
\end{align}
which upweights denoising at low noise levels, encouraging accurate reconstruction of fine structural details.

\paragraph{Training Strategy.}
The full SVD UNet is finetuned, with the option to restrict gradient updates to only the temporal transformer blocks after an initial warm-up period. This curriculum strategy first allows the model to adapt its full capacity to the semantic domain, then focuses optimization on the temporal layers that govern inter-frame consistency. The Semantic VAE, RGB VAE, and CLIP encoder all remain frozen throughout Stage~1 training.

\paragraph{Design Rationale.}
Separating semantic prediction from RGB generation provides several advantages:
\begin{itemize}
    \item \textbf{Reduced learning complexity:} The model learns scene \emph{structure} (where objects are and how they move) without simultaneously learning appearance (textures, lighting, colors).
    \item \textbf{Structural consistency:} Operating in semantic space enforces that generated sequences maintain valid scene layouts.
    \item \textbf{Parallel training:} Stages~1 and~2 can be trained independently, since Stage~2 uses ground-truth semantic maps during training.
    \item \textbf{Modular design:} The semantic prediction module can be improved or replaced independently of the video generation module.
\end{itemize}
This decomposition mirrors the strategy of Ctrl-V~\cite{luo2025ctrlv}, where high-level structure (bounding box trajectories) is modeled before photorealistic synthesis.

\subsection{Stage~2: Semantic-to-Video Generation}
\label{subsec:stage2}

Stage~2 generates photorealistic RGB video conditioned on semantic segmentation maps. This stage uses a video-adapted ControlNet architecture~\cite{zhang2023adding} that injects semantic conditioning into the frozen SVD backbone, learning to translate semantic layouts into realistic driving scenes while preserving temporal coherence.

\paragraph{Architectural Overview.}
Stage~2 consists of two parallel processing branches that share the same time embeddings and cross-attention conditioning:

\begin{enumerate}
    \item \textbf{Frozen SVD Backbone} ($\theta$): The pretrained SVD UNet, kept entirely frozen. It receives the noisy RGB video latents concatenated with the initial frame conditioning and produces the denoised video prediction. The backbone preserves the high-quality video generation prior learned from large-scale data.

    \item \textbf{Trainable ControlNet} ($\xi$): A copy of the SVD encoder and mid-block, initialized from the pretrained UNet weights. It receives the same 8-channel input as the backbone \emph{plus} the semantic latents as an additional control signal, and produces residual features that are injected into the frozen UNet decoder via zero-initialized convolution layers (see Section~\ref{subsec:controlnet}).
\end{enumerate}

\paragraph{Input Formation.}
The inputs to Stage~2 are prepared as follows:

\textit{RGB video encoding.} Ground-truth RGB video clips are encoded frame-by-frame using the frozen RGB VAE:
\begin{align}
    \mathbf{z}_0 = \text{VAE}_{\text{enc}}(\mathbf{f}_{1:T}) \in \mathbb{R}^{B \times T \times 4 \times h \times w},
    \label{eq:rgb_latent_enc}
\end{align}
and scaled by the VAE scaling factor. Gaussian noise is added to produce noisy latents $\hat{\mathbf{z}}_t$ following the EDM schedule, identical to Equation~\eqref{eq:noisy_semantic} but applied to RGB latents.

\textit{Initial frame conditioning.} The initial RGB frame is encoded with the RGB VAE and temporally replicated to $T$ frames:
\begin{align}
    \mathbf{z}^{(0)}_{\text{pad}} = \text{Repeat}\!\left(\text{VAE}_{\text{enc}}(\mathbf{f}^{(0)}),\, T\right) \in \mathbb{R}^{B \times T \times 4 \times h \times w}.
    \label{eq:frame_padding_stage2}
\end{align}
This is concatenated with the scaled noisy RGB latents along the channel dimension:
\begin{align}
    \mathbf{x}_{\text{in}} = \left[\hat{\mathbf{z}}_t / \sqrt{\sigma^2 + 1} \;\|\; \mathbf{z}^{(0)}_{\text{pad}}\right] \in \mathbb{R}^{B \times T \times 8 \times h \times w},
    \label{eq:unet_input_stage2}
\end{align}
providing an 8-channel input to both the frozen UNet backbone and the ControlNet.

\textit{CLIP conditioning.} The initial RGB frame is processed through the frozen CLIP vision encoder (Equation~\ref{eq:clip_embedding}) and the resulting embedding is repeated across all $T$ frames. This embedding is used as key and value inputs in the cross-attention layers of both the frozen UNet and the ControlNet, providing global scene context.

\textit{Semantic conditioning for ControlNet.} Ground-truth semantic segmentation maps are encoded using the frozen Semantic VAE:
\begin{align}
    \mathbf{s} = \text{SemVAE}_{\text{enc}}(\mathbf{y}_{1:T}) \in \mathbb{R}^{B \times T \times 4 \times h \times w}.
    \label{eq:semantic_cond_stage2}
\end{align}
These semantic latents serve as the \emph{control condition} that is exclusive to the ControlNet branch.

\paragraph{ControlNet Forward Pass.}
The ControlNet processes its inputs through two parallel convolutions at the input layer:
\begin{align}
    \mathbf{h}_0 = \text{Conv}_{\text{in}}^{8 \rightarrow 320}(\mathbf{x}_{\text{in}}) + \text{Conv}_{\text{ctrl}}^{4 \rightarrow 320}(\mathbf{s}),
    \label{eq:controlnet_input}
\end{align}
where $\text{Conv}_{\text{in}}$ is the standard UNet input convolution (copied from the pretrained weights) operating on the 8-channel concatenated input, and $\text{Conv}_{\text{ctrl}}$ is a separate convolution that projects the 4-channel semantic latents into the same 320-dimensional feature space. The semantic features are added to the main branch features, providing the control signal at the earliest network layer.

The combined features then pass through the ControlNet's encoder blocks and mid-block---which have the same architecture as the SVD UNet encoder---producing residual features at each resolution level. These residuals are passed through zero-initialized $1 \times 1$ convolution layers and scaled by a conditioning scale factor $\gamma$:
\begin{align}
    \mathbf{r}_i &= \gamma \cdot \mathcal{Z}_i\!\left(\mathbf{h}_i^{\text{ctrl}}\right), \quad i \in \{1, \ldots, N_{\text{down}}\}, \label{eq:controlnet_residuals} \\
    \mathbf{r}_{\text{mid}} &= \gamma \cdot \mathcal{Z}_{\text{mid}}\!\left(\mathbf{h}_{\text{mid}}^{\text{ctrl}}\right), \label{eq:controlnet_mid_residual}
\end{align}
where $\mathcal{Z}_i$ denotes the zero-initialized convolution at level $i$ and $\gamma$ is a scalar conditioning scale (default $\gamma = 1.0$). The zero initialization ensures the ControlNet contributes nothing at the start of training, preserving the pretrained SVD behavior.

\paragraph{Frozen UNet Forward Pass with ControlNet Injection.}
The frozen SVD UNet processes the same 8-channel input $\mathbf{x}_{\text{in}}$ through its encoder blocks, producing intermediate features $\{\mathbf{h}_i^{\text{unet}}\}$. At each decoder level, the ControlNet residuals are \emph{added} to the corresponding skip connections:
\begin{align}
    \tilde{\mathbf{h}}_i = \mathbf{h}_i^{\text{unet}} + \mathbf{r}_i.
    \label{eq:skip_injection}
\end{align}
Similarly, the mid-block output receives the mid-block residual: $\tilde{\mathbf{h}}_{\text{mid}} = \mathbf{h}_{\text{mid}}^{\text{unet}} + \mathbf{r}_{\text{mid}}$. The modified features are then decoded by the frozen UNet decoder to produce the final noise prediction.

The complete noise prediction can thus be written as:
\begin{align}
    \boldsymbol{\epsilon}_{\theta,\xi}(\hat{\mathbf{z}}_t, t, \mathbf{z}^{(0)}_{\text{pad}}, \mathbf{c}^{(0)}, \mathbf{s}) = \mathbb{U}_\theta\!\left(\mathbf{x}_{\text{in}}, t, \mathbf{c}^{(0)}_{\text{rep}}, \mathbf{a};\; \{\mathbf{r}_i\}_\xi,\, \mathbf{r}_{\text{mid},\xi}\right),
    \label{eq:ctrlv_box2video}
\end{align}
where $\mathbb{U}_\theta$ is the frozen UNet with ControlNet residuals injected at the skip connections, $\theta$ denotes the frozen backbone parameters, and $\xi$ denotes the trainable ControlNet parameters.

\paragraph{Training Objective.}
The training follows the same EDM-preconditioned objective as Stage~1:
\begin{align}
    \mathcal{L}_{\text{Stage2}} = \mathbb{E}_{t,\, \mathbf{z}_0,\, \boldsymbol{\epsilon}}\!\left[w(\sigma) \cdot \left\|\hat{\mathbf{z}}_{\text{denoised}} - \mathbf{z}_0\right\|^2\right],
    \label{eq:stage2_loss}
\end{align}
where $\hat{\mathbf{z}}_{\text{denoised}}$ is computed using the EDM skip connections (Equation~\ref{eq:edm_denoised_stage1} applied to RGB latents). Only the ControlNet parameters $\xi$ are updated; the SVD UNet backbone $\theta$, VAE, and CLIP encoder remain frozen.

\paragraph{Training Strategy.}
A critical design choice is that Stage~2 uses \emph{ground-truth} semantic maps as conditioning during training, not the predictions from Stage~1. This decoupling provides two advantages:
\begin{enumerate}
    \item \textbf{Parallel training:} Stages~1 and~2 can be optimized independently and simultaneously, since Stage~2 does not depend on Stage~1 predictions during training.
    \item \textbf{Training stability:} Ground-truth conditioning avoids the compounding of errors from imperfect Stage~1 predictions during the learning process.
\end{enumerate}
At \emph{inference} time, the pipeline operates sequentially: Stage~1 generates the semantic video sequence from the initial frame, and Stage~2 then generates the photorealistic RGB video conditioned on the Stage~1 output.

\paragraph{Why Freeze the SVD Backbone?}
Freezing the SVD UNet during Stage~2 training serves multiple purposes:
\begin{itemize}
    \item \textbf{Preserves the pretrained video prior:} The backbone retains its learned knowledge of natural video dynamics, appearance, and temporal coherence.
    \item \textbf{Prevents catastrophic forgetting:} Training only the ControlNet prevents degradation of the pretrained generation quality.
    \item \textbf{Parameter efficiency:} Only the ControlNet parameters are updated, reducing GPU memory requirements and accelerating convergence.
    \item \textbf{Stable conditioning learning:} The ControlNet learns to \emph{steer} the frozen backbone rather than retrain it, inheriting the stability guarantees of the zero-initialized architecture (Section~\ref{subsec:controlnet}).
\end{itemize}

\paragraph{Full Inference Pipeline.}
At inference, the complete pipeline operates as follows:
\begin{enumerate}
    \item \textbf{Input:} A single initial RGB frame $\mathbf{f}^{(0)}$.
    \item \textbf{Stage~1:} The SVD UNet generates a sequence of semantic latents $\mathbf{s}_{1:T}$ conditioned on $\mathbf{f}^{(0)}$.
    \item \textbf{Stage~2:} The frozen SVD backbone with ControlNet generates RGB video latents $\mathbf{z}_{1:T}$ conditioned on both $\mathbf{f}^{(0)}$ and the predicted $\mathbf{s}_{1:T}$.
    \item \textbf{Decoding:} The RGB VAE decoder maps $\mathbf{z}_{1:T}$ back to pixel space, producing the final video $\hat{\mathbf{f}}_{1:T}$.
\end{enumerate}

\paragraph{Summary of the Two-Stage Design.}
The key advantages of this decomposed architecture are:
\begin{itemize}
    \item \textbf{Structural control:} Semantic maps provide explicit, pixel-level scene specification.
    \item \textbf{Modular stages:} Each stage can be trained, evaluated, and improved independently.
    \item \textbf{Parallel training:} Ground-truth semantic conditioning enables simultaneous optimization.
    \item \textbf{Strong temporal consistency:} Temporal attention in both stages ensures coherent frame-to-frame transitions.
    \item \textbf{Efficient adaptation:} Frozen backbones with lightweight adapters (finetuned temporal layers in Stage~1, ControlNet in Stage~2) enable training with limited computational resources.
\end{itemize}

% -----------------------------------------------------------------
\section{Loss Functions}
\label{sec:losses}

\subsection{Semantic VAE Losses}
\label{subsec:vae_losses}

As established in Section~\ref{subsec:semantic_native_vae}, the Semantic-Native VAE is trained with cross-entropy as the primary objective (Equation~\ref{eq:ce_loss}). In practice, we augment this base loss with two refinements: boundary weighting and Dice loss.

\paragraph{Boundary-Weighted Cross-Entropy.}
Standard cross-entropy treats all pixels equally, which causes the model to focus on large homogeneous regions (e.g., road, sky) while neglecting thin structures (e.g., poles, traffic signs) and class boundaries. To address this, we introduce pixel-wise weights based on a boundary mask:
\begin{align}
    \mathcal{L}_{\text{BCE}} = \frac{\sum_{i \in \mathcal{V}} w_i \cdot \ell_{\text{CE}}(\hat{\mathbf{s}}_i, y_i)}{\sum_{i \in \mathcal{V}} w_i}, \quad w_i = 1 + \alpha \cdot b_i,
    \label{eq:bce_loss}
\end{align}
where $\ell_{\text{CE}}(\hat{\mathbf{s}}_i, y_i) = -\log\!\left(\text{softmax}(\hat{\mathbf{s}}_i)\right)_{y_i}$ is the per-pixel cross-entropy, $b_i \in \{0, 1\}$ is the boundary mask, $\alpha$ is the boundary emphasis factor, and $\mathcal{V}$ is the set of valid (non-ignored) pixels.

The boundary mask is computed efficiently via max-min pooling with a $3 \times 3$ kernel:
\begin{align}
    b_i = \begin{cases} 1 & \text{if } \max_{j \in \mathcal{N}(i)} y_j \neq \min_{j \in \mathcal{N}(i)} y_j, \\ 0 & \text{otherwise,}\end{cases}
    \label{eq:boundary_mask}
\end{align}
where $\mathcal{N}(i)$ is the $3 \times 3$ neighborhood of pixel $i$. This identifies pixels at class transitions without requiring explicit edge detection. With $\alpha = 4.0$, boundary pixels receive $5\times$ the loss weight of interior pixels, encouraging the model to preserve sharp class boundaries.

\paragraph{Dice Loss.}
To address class imbalance---vegetation may occupy over $1{,}000\times$ more pixels than traffic signs---we employ the soft Dice loss~\cite{milletari2016vnet}:
\begin{align}
    \mathcal{L}_{\text{Dice}} = 1 - \frac{1}{C} \sum_{c=1}^{C} \frac{2 \sum_i p_{i,c} \cdot t_{i,c} + \epsilon}{\sum_i p_{i,c} + \sum_i t_{i,c} + \epsilon},
    \label{eq:dice_loss}
\end{align}
where $p_{i,c} = \text{softmax}(\hat{\mathbf{s}}_i)_c$ is the predicted probability for class $c$ at pixel $i$, $t_{i,c}$ is the one-hot ground truth, and $\epsilon = 1$ is a smoothing constant. The Dice loss computes a per-class overlap measure and averages across classes, thereby treating all classes equally regardless of their spatial prevalence.

\paragraph{Combined Loss.}
The total loss for Semantic VAE training is:
\begin{align}
    \mathcal{L}_{\text{VAE}} = \mathcal{L}_{\text{BCE}} + \lambda_{\text{Dice}} \cdot \mathcal{L}_{\text{Dice}},
    \label{eq:total_vae_loss}
\end{align}
where $\lambda_{\text{Dice}} = 0.5$ balances the two objectives. The ablation of these loss components is presented in Section~\ref{subsec:loss_ablation}.

\subsection{Diffusion Training Losses}
\label{subsec:diffusion_losses}

Both Stage~1 and Stage~2 use the standard diffusion denoising objective with EDM noise scheduling~\cite{karras2022elucidating}. The loss is a weighted mean-squared error between the predicted and actual noise:
\begin{align}
    \mathcal{L}_{\text{diffusion}} = \mathbb{E}_{t \sim p(t), \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})}\!\left[w(t) \cdot \left\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t, \mathbf{c})\right\|^2\right],
    \label{eq:diffusion_loss}
\end{align}
where $w(t)$ is a time-dependent weighting function defined by the EDM schedule and $\mathbf{c}$ represents the conditioning inputs (initial frame, CLIP embedding, and---for Stage~2---semantic latents).
